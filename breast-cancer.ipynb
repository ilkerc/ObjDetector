{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import time\n",
    "# os.environ['THEANO_FLAGS']='contexts=dev0->cuda0;dev1->cuda1'\n",
    "os.environ['THEANO_FLAGS']='device=cpu'\n",
    "import theano\n",
    "import lasagne\n",
    "import numpy as np\n",
    "import cPickle as pickle\n",
    "import theano.tensor as T\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits as load_sk_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "from lasagne.layers import InputLayer, DenseLayer, DropoutLayer\n",
    "from helpers.DiscreteLayer import DiscreteLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breast Cancer dataset cl\n",
    "* 30 Dimensions of Features\n",
    "* 2 Classes\n",
    "* 569 examples (212(M),357(B))\n",
    "\n",
    "I'll be using ML.Perceptron, for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "NUM_EPOCHS = 1500\n",
    "BATCH_SIZE = 10\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# Disc. Layer Settings\n",
    "DISC = True\n",
    "QUANT_UNIT = 5\n",
    "QUANT = np.array(np.repeat(0.125, QUANT_UNIT), dtype='float32')\n",
    "VARIANCE_DEVIDER = 4.0\n",
    "ADDITIONAL_COST = True\n",
    "\n",
    "# Test Specs\n",
    "TEST_NAME = 'breast-disc'\n",
    "\n",
    "# Additional Settings\n",
    "lasagne.random.set_rng(np.random.RandomState(12345))  # Set random state so we can investigate results\n",
    "np.random.seed(1234)\n",
    "#theano.config.exception_verbosity = 'high'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    dataset = load_sk_dataset()\n",
    "    X = dataset['data']\n",
    "    no_classes = len(np.unique(dataset['target']))\n",
    "    X = normalize(X, norm='l2')\n",
    "    #     X = (X - X.mean()) / X.std()\n",
    "    Y = dataset['target']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=41)\n",
    "    return dict(X_train=X_train,\n",
    "               y_train=y_train,\n",
    "               X_test=X_test,\n",
    "               y_test=y_test,\n",
    "               classes=no_classes)\n",
    "data = load_dataset()\n",
    "NUM_CLASSES = data['classes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Discret. Layer\n"
     ]
    }
   ],
   "source": [
    "def build_mlp(Xs, disc, classnum, QUANT_UNIT):\n",
    "    tanh = lasagne.nonlinearities.tanh\n",
    "    softmax = lasagne.nonlinearities.softmax\n",
    "    \n",
    "    l_in = InputLayer(shape=Xs.shape)\n",
    "    l_dense1 = DenseLayer(l_in, num_units=25, nonlinearity=tanh)\n",
    "    l_dense2 = DenseLayer(l_dense1, num_units=QUANT_UNIT, nonlinearity=tanh, name='param_regressor')\n",
    "    if disc:\n",
    "        sharedBins = theano.shared(None, name='sharedBins')\n",
    "        l_dis = DiscreteLayer(l_dense2, sharedBins=sharedBins, name='disclayer')\n",
    "        print(\"Using Discret. Layer\")\n",
    "    else:\n",
    "        l_dis = l_dense2\n",
    "        print(\"No Disc. Layer\")\n",
    "    l_class = DenseLayer(l_dis, num_units=classnum, nonlinearity=softmax)\n",
    "    \n",
    "    if disc:\n",
    "        return l_class, sharedBins\n",
    "    else:\n",
    "        return l_class\n",
    "\n",
    "if DISC:\n",
    "    model, sharedBins = build_mlp(data['X_train'], disc=DISC, classnum=NUM_CLASSES, QUANT_UNIT=QUANT_UNIT)\n",
    "else:\n",
    "    model = build_mlp(data['X_train'], disc=DISC, classnum=NUM_CLASSES, QUANT_UNIT=QUANT_UNIT)\n",
    "model_params = lasagne.layers.get_all_params(model, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input: dist, dist.shape = (-1, num_units)\n",
    "Find quantization bins of a given dist history\n",
    "Returns a list of (x, num_units), where x's length is a random variable\n",
    "\"\"\"\n",
    "def find_quantization_bins(dist, sharedBins):\n",
    "    # Quantizer function\n",
    "    def Q(x, y):\n",
    "        return y * np.floor((x/y) + .5)\n",
    "    \n",
    "    shape = dist.shape\n",
    "    init_Q = QUANT\n",
    "    final_Q = []\n",
    "    \n",
    "    # Theta iterator\n",
    "    for i in range(shape[1]):\n",
    "        theta_i = dist[:, i]\n",
    "        \n",
    "        # Whats is the error threshold for this distribution\n",
    "        Q_eps = np.var(theta_i) / VARIANCE_DEVIDER\n",
    "        \n",
    "        # Batch Iterator\n",
    "        final_Q_i = []\n",
    "        for j in range(shape[0]):\n",
    "            theta = theta_i[j]\n",
    "            \n",
    "            # Quantized theta = Quantization bins\n",
    "            q = init_Q[i]\n",
    "            x_i = theta\n",
    "            x_o = Q(x_i, q)\n",
    "            \n",
    "            # Optimize x_o\n",
    "\n",
    "            while(np.abs(x_o - x_i) > Q_eps):\n",
    "                q = q / 2\n",
    "                x_o = Q(x_i, q)\n",
    "            \n",
    "            # End of optimisation\n",
    "            final_Q_i.append(x_o)\n",
    "        \n",
    "        # Append to outer list\n",
    "        uniques = np.unique(np.array(final_Q_i))\n",
    "        final_Q.append(uniques.astype(theano.config.floatX))\n",
    "        \n",
    "    # Report\n",
    "    print \"New Bin Sizes: [\" + \", \".join([str(final_Q[x].shape[0]) for x in range(shape[1])] ) + \"]\"\n",
    "    sharedBins.set_value(final_Q)\n",
    "    return final_Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_functions():\n",
    "    X = T.matrix(dtype=theano.config.floatX)\n",
    "    y = T.ivector()\n",
    "\n",
    "    ## Layer History\n",
    "    if DISC:\n",
    "        l_disc = next(l for l in lasagne.layers.get_all_layers(model) if l.name is 'disclayer')\n",
    "        l_paramreg = next(l for l in lasagne.layers.get_all_layers(model) if l.name is 'param_regressor')\n",
    "        l_disc_output, l_paramreg_output = lasagne.layers.get_output([l_disc, l_paramreg], X, deterministic=False)\n",
    "    ## Layer History\n",
    "\n",
    "    # training output\n",
    "    output_train = lasagne.layers.get_output(model, X, deterministic=False)\n",
    "\n",
    "    # evaluation output. Also includes output of transform for plotting\n",
    "    output_eval = lasagne.layers.get_output(model, X, deterministic=True)\n",
    "\n",
    "    sh_lr = theano.shared(lasagne.utils.floatX(LEARNING_RATE))\n",
    "    cost = T.mean(T.nnet.categorical_crossentropy(output_train, y))\n",
    "    \n",
    "    if ADDITIONAL_COST and DISC:\n",
    "        cost += T.mean(lasagne.objectives.squared_error(l_disc_output, l_paramreg_output))\n",
    "    \n",
    "    updates = lasagne.updates.adam(cost, model_params, learning_rate=sh_lr)\n",
    "    \n",
    "    # updates = lasagne.updates.nesterov_momentum(cost, model_params, learning_rate=0.01, momentum=0.9)\n",
    "\n",
    "    if DISC:\n",
    "        train = theano.function([X, y], [cost, output_train, l_disc_output, l_paramreg_output], updates=updates, allow_input_downcast=True)\n",
    "    else:\n",
    "        train = theano.function([X, y], [cost, output_train], updates=updates, allow_input_downcast=True)\n",
    "    eval = theano.function([X], [output_eval], allow_input_downcast=True)\n",
    "    \n",
    "    return train, eval, sh_lr\n",
    "\n",
    "train, eval, sh_lr = build_functions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_epoch(X, y):\n",
    "    # History Keeping\n",
    "    param_output = []\n",
    "    disc_output = []\n",
    "    # History\n",
    "    num_samples = X.shape[0]\n",
    "    num_batches = int(np.ceil(num_samples / float(BATCH_SIZE)))\n",
    "    costs = []\n",
    "    correct = 0\n",
    "    for i in range(num_batches):\n",
    "        idx = range(i*BATCH_SIZE, np.minimum((i+1)*BATCH_SIZE, num_samples))\n",
    "        X_batch = X[idx]\n",
    "        y_batch = y[idx]\n",
    "        if DISC:\n",
    "            cost, output_train, l_disc_output, l_paramreg_output = train(X_batch, y_batch)\n",
    "            param_output = np.append(param_output, l_paramreg_output)\n",
    "            disc_output = np.append(disc_output, l_disc_output)\n",
    "        else:\n",
    "            cost, output_train = train(X_batch, y_batch)\n",
    "        costs += [cost]\n",
    "        preds = np.argmax(output_train, axis=-1)\n",
    "        correct += np.sum(y_batch == preds)\n",
    "    \n",
    "    return np.mean(costs), correct / float(num_samples), param_output, disc_output\n",
    "\n",
    "\n",
    "def eval_epoch(X, y):\n",
    "    output_eval = eval(X)\n",
    "    preds = np.argmax(output_eval, axis=-1)\n",
    "    acc = np.mean(preds == y)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: T.cost 1.517941, Train acc 0.487126, test acc 0.719444, took 0.123 sec.\n",
      "Epoch 1: T.cost 0.721206, Train acc 0.798191, test acc 0.855556, took 0.0801 sec.\n",
      "Epoch 2: T.cost 0.477164, Train acc 0.871955, test acc 0.861111, took 0.065 sec.\n",
      "Epoch 3: T.cost 0.381969, Train acc 0.890049, test acc 0.875000, took 0.0634 sec.\n",
      "Epoch 4: T.cost 0.320268, Train acc 0.907446, test acc 0.883333, took 0.0473 sec.\n",
      "Epoch 5: T.cost 0.281332, Train acc 0.920668, test acc 0.872222, took 0.051 sec.\n",
      "Epoch 6: T.cost 0.250034, Train acc 0.931106, test acc 0.875000, took 0.0641 sec.\n",
      "Epoch 7: T.cost 0.223263, Train acc 0.940153, test acc 0.875000, took 0.0588 sec.\n",
      "Epoch 8: T.cost 0.205542, Train acc 0.942937, test acc 0.880556, took 0.052 sec.\n",
      "Epoch 9: T.cost 0.188718, Train acc 0.948504, test acc 0.888889, took 0.0564 sec.\n",
      "New Bin Sizes: [17, 17, 17, 17, 17]\n",
      "Epoch 10: T.cost 0.184676, Train acc 0.945024, test acc 0.897222, took 0.104 sec.\n",
      "Epoch 11: T.cost 0.175055, Train acc 0.954767, test acc 0.894444, took 0.17 sec.\n",
      "Epoch 12: T.cost 0.179107, Train acc 0.951287, test acc 0.900000, took 0.176 sec.\n",
      "Epoch 13: T.cost 0.161293, Train acc 0.955463, test acc 0.900000, took 0.18 sec.\n",
      "Epoch 14: T.cost 0.143905, Train acc 0.959638, test acc 0.905556, took 0.182 sec.\n",
      "Epoch 15: T.cost 0.155265, Train acc 0.955463, test acc 0.911111, took 0.178 sec.\n",
      "Epoch 16: T.cost 0.137715, Train acc 0.958942, test acc 0.888889, took 0.191 sec.\n",
      "Epoch 17: T.cost 0.147379, Train acc 0.961030, test acc 0.888889, took 0.186 sec.\n",
      "Epoch 18: T.cost 0.137891, Train acc 0.957550, test acc 0.897222, took 0.182 sec.\n",
      "New LR: 0.00899999979883\n",
      "Epoch 19: T.cost 0.127096, Train acc 0.963118, test acc 0.927778, took 0.163 sec.\n",
      "New Bin Sizes: [17, 17, 17, 17, 17]\n",
      "Epoch 20: T.cost 0.110723, Train acc 0.970077, test acc 0.913889, took 0.16 sec.\n",
      "Epoch 21: T.cost 0.104172, Train acc 0.972164, test acc 0.905556, took 0.127 sec.\n",
      "Epoch 22: T.cost 0.110217, Train acc 0.967293, test acc 0.905556, took 0.159 sec.\n",
      "Epoch 23: T.cost 0.109817, Train acc 0.971468, test acc 0.886111, took 0.183 sec.\n",
      "Epoch 24: T.cost 0.101890, Train acc 0.972164, test acc 0.927778, took 0.19 sec.\n",
      "Epoch 25: T.cost 0.103640, Train acc 0.969381, test acc 0.919444, took 0.127 sec.\n",
      "Epoch 26: T.cost 0.096625, Train acc 0.969381, test acc 0.936111, took 0.134 sec.\n",
      "Epoch 27: T.cost 0.080535, Train acc 0.976340, test acc 0.922222, took 0.128 sec.\n",
      "Epoch 28: T.cost 0.076876, Train acc 0.977731, test acc 0.944444, took 0.14 sec.\n",
      "Epoch 29: T.cost 0.059640, Train acc 0.984690, test acc 0.938889, took 0.128 sec.\n",
      "New Bin Sizes: [17, 17, 14, 17, 17]\n",
      "Epoch 30: T.cost 0.057156, Train acc 0.983994, test acc 0.919444, took 0.227 sec.\n",
      "Epoch 31: T.cost 0.061970, Train acc 0.983994, test acc 0.944444, took 0.128 sec.\n",
      "Epoch 32: T.cost 0.053684, Train acc 0.985386, test acc 0.938889, took 0.149 sec.\n",
      "Epoch 33: T.cost 0.054796, Train acc 0.984690, test acc 0.936111, took 0.128 sec.\n",
      "Epoch 34: T.cost 0.050192, Train acc 0.986778, test acc 0.941667, took 0.165 sec.\n",
      "Epoch 35: T.cost 0.061505, Train acc 0.983299, test acc 0.925000, took 0.198 sec.\n",
      "Epoch 36: T.cost 0.047078, Train acc 0.987474, test acc 0.927778, took 0.164 sec.\n",
      "Epoch 37: T.cost 0.059403, Train acc 0.979819, test acc 0.922222, took 0.126 sec.\n",
      "Epoch 38: T.cost 0.046281, Train acc 0.988866, test acc 0.944444, took 0.129 sec.\n",
      "New LR: 0.00809999965131\n",
      "Epoch 39: T.cost 0.044652, Train acc 0.989562, test acc 0.950000, took 0.127 sec.\n",
      "New Bin Sizes: [17, 17, 12, 17, 17]\n",
      "Epoch 40: T.cost 0.033670, Train acc 0.988866, test acc 0.938889, took 0.162 sec.\n",
      "Epoch 41: T.cost 0.045248, Train acc 0.983994, test acc 0.919444, took 0.127 sec.\n",
      "Epoch 42: T.cost 0.049630, Train acc 0.983299, test acc 0.922222, took 0.128 sec.\n",
      "Epoch 43: T.cost 0.062722, Train acc 0.982603, test acc 0.944444, took 0.127 sec.\n",
      "Epoch 44: T.cost 0.054200, Train acc 0.986082, test acc 0.955556, took 0.128 sec.\n",
      "Epoch 45: T.cost 0.029759, Train acc 0.990953, test acc 0.955556, took 0.171 sec.\n",
      "Epoch 46: T.cost 0.033828, Train acc 0.992345, test acc 0.952778, took 0.128 sec.\n",
      "Epoch 47: T.cost 0.027815, Train acc 0.993041, test acc 0.955556, took 0.126 sec.\n",
      "Epoch 48: T.cost 0.020217, Train acc 0.995825, test acc 0.958333, took 0.173 sec.\n",
      "Epoch 49: T.cost 0.016708, Train acc 0.997216, test acc 0.950000, took 0.128 sec.\n",
      "New Bin Sizes: [17, 17, 10, 17, 17]\n",
      "Epoch 50: T.cost 0.015137, Train acc 0.997216, test acc 0.955556, took 0.16 sec.\n",
      "Epoch 51: T.cost 0.013746, Train acc 0.997216, test acc 0.955556, took 0.127 sec.\n",
      "Epoch 52: T.cost 0.012980, Train acc 0.997216, test acc 0.952778, took 0.129 sec.\n",
      "Epoch 53: T.cost 0.012060, Train acc 0.997912, test acc 0.950000, took 0.127 sec.\n",
      "Epoch 54: T.cost 0.011628, Train acc 0.997912, test acc 0.955556, took 0.128 sec.\n",
      "Epoch 55: T.cost 0.010708, Train acc 0.997912, test acc 0.958333, took 0.127 sec.\n",
      "Epoch 56: T.cost 0.010853, Train acc 0.997912, test acc 0.952778, took 0.128 sec.\n",
      "Epoch 57: T.cost 0.010734, Train acc 0.997912, test acc 0.961111, took 0.127 sec.\n",
      "Epoch 58: T.cost 0.075256, Train acc 0.974948, test acc 0.955556, took 0.128 sec.\n",
      "New LR: 0.00728999935091\n",
      "Epoch 59: T.cost 0.101126, Train acc 0.973556, test acc 0.930556, took 0.129 sec.\n",
      "New Bin Sizes: [17, 17, 10, 17, 17]\n",
      "Epoch 60: T.cost 0.076780, Train acc 0.975644, test acc 0.950000, took 0.159 sec.\n",
      "Epoch 61: T.cost 0.045909, Train acc 0.987474, test acc 0.966667, took 0.127 sec.\n",
      "Epoch 62: T.cost 0.015011, Train acc 0.995825, test acc 0.966667, took 0.129 sec.\n",
      "Epoch 63: T.cost 0.013002, Train acc 0.997912, test acc 0.966667, took 0.127 sec.\n",
      "Epoch 64: T.cost 0.010942, Train acc 0.997912, test acc 0.961111, took 0.128 sec.\n",
      "Epoch 65: T.cost 0.010508, Train acc 0.997912, test acc 0.963889, took 0.127 sec.\n",
      "Epoch 66: T.cost 0.009556, Train acc 0.997912, test acc 0.961111, took 0.128 sec.\n",
      "Epoch 67: T.cost 0.009038, Train acc 0.997912, test acc 0.958333, took 0.128 sec.\n",
      "Epoch 68: T.cost 0.008959, Train acc 0.997912, test acc 0.961111, took 0.128 sec.\n",
      "Epoch 69: T.cost 0.008651, Train acc 0.997912, test acc 0.963889, took 0.128 sec.\n",
      "New Bin Sizes: [17, 17, 10, 17, 17]\n",
      "Epoch 70: T.cost 0.008540, Train acc 0.997912, test acc 0.963889, took 0.16 sec.\n",
      "Epoch 71: T.cost 0.008341, Train acc 0.997912, test acc 0.958333, took 0.127 sec.\n",
      "Epoch 72: T.cost 0.016205, Train acc 0.993737, test acc 0.925000, took 0.129 sec.\n",
      "Epoch 73: T.cost 0.081992, Train acc 0.975644, test acc 0.955556, took 0.127 sec.\n",
      "Epoch 74: T.cost 0.029026, Train acc 0.990257, test acc 0.950000, took 0.128 sec.\n",
      "Epoch 75: T.cost 0.013783, Train acc 0.995129, test acc 0.958333, took 0.128 sec.\n",
      "Epoch 76: T.cost 0.010224, Train acc 0.997912, test acc 0.955556, took 0.128 sec.\n",
      "Epoch 77: T.cost 0.011548, Train acc 0.997912, test acc 0.961111, took 0.127 sec.\n",
      "Epoch 78: T.cost 0.007218, Train acc 0.998608, test acc 0.961111, took 0.13 sec.\n",
      "New LR: 0.00656099924818\n",
      "Epoch 79: T.cost 0.008250, Train acc 0.998608, test acc 0.952778, took 0.152 sec.\n",
      "New Bin Sizes: [17, 17, 10, 16, 17]\n",
      "Epoch 80: T.cost 0.026771, Train acc 0.990953, test acc 0.908333, took 0.235 sec.\n",
      "Epoch 81: T.cost 0.020784, Train acc 0.995825, test acc 0.955556, took 0.137 sec.\n",
      "Epoch 82: T.cost 0.007329, Train acc 0.997912, test acc 0.961111, took 0.148 sec.\n",
      "Epoch 83: T.cost 0.006966, Train acc 0.998608, test acc 0.961111, took 0.129 sec.\n",
      "Epoch 84: T.cost 0.006617, Train acc 0.999304, test acc 0.955556, took 0.146 sec.\n",
      "Epoch 85: T.cost 0.005662, Train acc 0.999304, test acc 0.955556, took 0.129 sec.\n",
      "Epoch 86: T.cost 0.005196, Train acc 0.999304, test acc 0.958333, took 0.147 sec.\n",
      "Epoch 87: T.cost 0.004538, Train acc 0.999304, test acc 0.952778, took 0.129 sec.\n",
      "Epoch 88: T.cost 0.004245, Train acc 0.999304, test acc 0.958333, took 0.16 sec.\n",
      "Epoch 89: T.cost 0.003898, Train acc 0.999304, test acc 0.961111, took 0.188 sec.\n",
      "New Bin Sizes: [17, 17, 8, 16, 17]\n",
      "Epoch 90: T.cost 0.003634, Train acc 0.999304, test acc 0.963889, took 0.267 sec.\n",
      "Epoch 91: T.cost 0.003451, Train acc 0.999304, test acc 0.961111, took 0.178 sec.\n",
      "Epoch 92: T.cost 0.003136, Train acc 0.999304, test acc 0.961111, took 0.158 sec.\n",
      "Epoch 93: T.cost 0.002937, Train acc 0.999304, test acc 0.961111, took 0.155 sec.\n",
      "Epoch 94: T.cost 0.002755, Train acc 0.999304, test acc 0.963889, took 0.132 sec.\n",
      "Epoch 95: T.cost 0.002479, Train acc 0.999304, test acc 0.963889, took 0.132 sec.\n",
      "Epoch 96: T.cost 0.002385, Train acc 1.000000, test acc 0.961111, took 0.145 sec.\n",
      "Epoch 97: T.cost 0.002270, Train acc 1.000000, test acc 0.963889, took 0.155 sec.\n",
      "Epoch 98: T.cost 0.002106, Train acc 1.000000, test acc 0.961111, took 0.183 sec.\n",
      "New LR: 0.005904899491\n",
      "Epoch 99: T.cost 0.002044, Train acc 1.000000, test acc 0.958333, took 0.19 sec.\n",
      "New Bin Sizes: [17, 17, 7, 16, 17]\n",
      "Epoch 100: T.cost 0.001934, Train acc 1.000000, test acc 0.958333, took 0.169 sec.\n",
      "Epoch 101: T.cost 0.001976, Train acc 1.000000, test acc 0.961111, took 0.135 sec.\n",
      "Epoch 102: T.cost 0.001933, Train acc 1.000000, test acc 0.958333, took 0.136 sec.\n",
      "Epoch 103: T.cost 0.063025, Train acc 0.984690, test acc 0.897222, took 0.138 sec.\n",
      "Epoch 104: T.cost 0.089639, Train acc 0.977731, test acc 0.952778, took 0.168 sec.\n",
      "Epoch 105: T.cost 0.020082, Train acc 0.993041, test acc 0.963889, took 0.149 sec.\n",
      "Epoch 106: T.cost 0.004193, Train acc 0.999304, test acc 0.955556, took 0.16 sec.\n",
      "Epoch 107: T.cost 0.002789, Train acc 1.000000, test acc 0.955556, took 0.151 sec.\n",
      "Epoch 108: T.cost 0.003058, Train acc 0.999304, test acc 0.947222, took 0.172 sec.\n",
      "Epoch 109: T.cost 0.002537, Train acc 1.000000, test acc 0.958333, took 0.187 sec.\n",
      "New Bin Sizes: [17, 17, 6, 15, 17]\n",
      "Epoch 110: T.cost 0.002264, Train acc 1.000000, test acc 0.955556, took 0.273 sec.\n",
      "Epoch 111: T.cost 0.002020, Train acc 1.000000, test acc 0.958333, took 0.212 sec.\n",
      "Epoch 112: T.cost 0.001797, Train acc 1.000000, test acc 0.958333, took 0.213 sec.\n",
      "Epoch 113: T.cost 0.001710, Train acc 1.000000, test acc 0.955556, took 0.207 sec.\n",
      "Epoch 114: T.cost 0.001644, Train acc 1.000000, test acc 0.955556, took 0.171 sec.\n",
      "Epoch 115: T.cost 0.001609, Train acc 1.000000, test acc 0.955556, took 0.177 sec.\n",
      "Epoch 116: T.cost 0.001545, Train acc 1.000000, test acc 0.955556, took 0.138 sec.\n",
      "Epoch 117: T.cost 0.001459, Train acc 1.000000, test acc 0.958333, took 0.137 sec.\n",
      "Epoch 118: T.cost 0.001384, Train acc 1.000000, test acc 0.958333, took 0.136 sec.\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(formatter={'float': '{: 0.4f}'.format}, suppress=True)\n",
    "train_accs, test_accs = [], []\n",
    "total_time = 0\n",
    "param_outputs, disc_outputs = [], []\n",
    "disc_dist_t_1 = None\n",
    "quantized_bins = []\n",
    "try:\n",
    "    for n in range(NUM_EPOCHS):\n",
    "        start_time = time.time()\n",
    "        train_cost, train_acc, param_output, disc_output = train_epoch(data['X_train'], data['y_train'])\n",
    "        test_acc = eval_epoch(data['X_test'], data['y_test'])\n",
    "        test_accs += [test_acc]\n",
    "        train_accs += [train_acc]\n",
    "\n",
    "        if DISC:\n",
    "            param_outputs = np.append(param_outputs, param_output)\n",
    "            disc_outputs = np.append(disc_outputs, disc_output)\n",
    "\n",
    "        if (n+1) % 20 == 0:\n",
    "            new_lr = sh_lr.get_value() * 0.90\n",
    "            print \"New LR:\", new_lr\n",
    "            sh_lr.set_value(lasagne.utils.floatX(new_lr))\n",
    "        \n",
    "        # Non-uniform Quantization\n",
    "        if DISC:\n",
    "            if n>0 and np.mod(n, 10) == 0:\n",
    "                dist = disc_output.reshape((-1, QUANT_UNIT))\n",
    "                q_bins = find_quantization_bins(dist, sharedBins=sharedBins)\n",
    "                quantized_bins.append(q_bins)\n",
    "\n",
    "        time_spent = time.time() - start_time\n",
    "        total_time += time_spent\n",
    "        print \"Epoch {0}: T.cost {1:0.6f}, Train acc {2:0.6f}, test acc {3:0.6f}, took {4:.3} sec.\".format(\n",
    "                n, train_cost, train_acc, test_acc, time_spent)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "print \"\\nTotal time spent: {0:.5} seconds\\nTraing Acc: {1}\\nTest Acc: {2}\\n\".format(total_time, train_acc, test_acc) \n",
    "\n",
    "if DISC:\n",
    "    story = {'train_accs': train_accs,\n",
    "             'test_accs': test_accs,\n",
    "             'epoch_reached': n, \n",
    "             'total_time': total_time,\n",
    "             'disc_enabled': DISC,\n",
    "             'learning_rate': LEARNING_RATE,\n",
    "             'batch_size': BATCH_SIZE,\n",
    "             'dense_params': param_output,\n",
    "             'disc_params': disc_output,\n",
    "             'quantized_bins': quantized_bins}\n",
    "else:\n",
    "    story = {'train_accs': train_accs,\n",
    "             'test_accs': test_accs,\n",
    "             'epoch_reached': n, \n",
    "             'total_time': total_time,\n",
    "             'disc_enabled': DISC,\n",
    "             'learning_rate': LEARNING_RATE,\n",
    "             'batch_size': BATCH_SIZE,\n",
    "             'disc_params': disc_output}   \n",
    "\n",
    "with open(TEST_NAME + '.model', 'wb') as fp:\n",
    "  pickle.dump(story, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HISTOGRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "disc_output_r = disc_output.reshape((-1, QUANT_UNIT))\n",
    "plt.figure()\n",
    "plt.plot(disc_output_r[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(1-np.array(train_accs), label='Training Error')\n",
    "plt.plot(1-np.array(test_accs), label='Test Error')\n",
    "plt.legend(fontsize=20)\n",
    "plt.xlabel('Epoch', fontsize=8)\n",
    "plt.ylabel('Error', fontsize=8)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:theano]",
   "language": "python",
   "name": "conda-env-theano-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
