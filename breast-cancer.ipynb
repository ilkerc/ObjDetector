{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.sandbox.cuda): CUDA is installed, but device gpu0 is not available  (error: Unable to get the number of gpus available: unknown error)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import time\n",
    "# os.environ['THEANO_FLAGS']='contexts=dev0->cuda0;dev1->cuda1'\n",
    "os.environ['THEANO_FLAGS']='device=gpu0'\n",
    "import theano\n",
    "import lasagne\n",
    "import numpy as np\n",
    "import cPickle as pickle\n",
    "import theano.tensor as T\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lasagne.layers import InputLayer, DenseLayer, DropoutLayer\n",
    "from helpers.DiscreteLayer import DiscreteLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breast Cancer dataset cl\n",
    "* 30 Dimensions of Features\n",
    "* 2 Classes\n",
    "* 569 examples (212(M),357(B))\n",
    "\n",
    "I'll be using ML.Perceptron, for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "NUM_EPOCHS = 1500\n",
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 0.005\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "# Disc. Layer Settings\n",
    "DISC = False\n",
    "QUANT = np.array([0.125, 0.125, 0.125, 0.125, 0.125], dtype='float32')\n",
    "QUANT_UNIT = QUANT.shape[0]\n",
    "VARIANCE_DEVIDER = 16.0\n",
    "\n",
    "# Test Specs\n",
    "TEST_NAME = 'breast-disc'\n",
    "\n",
    "# Additional Settings\n",
    "lasagne.random.set_rng(np.random.RandomState(12345))  # Set random state so we can investigate results\n",
    "np.random.seed(1234)\n",
    "#theano.config.exception_verbosity = 'high'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    dataset = load_breast_cancer()\n",
    "    X = dataset['data']\n",
    "    Y = dataset['target']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "    return dict(X_train=X_train,\n",
    "               y_train=y_train,\n",
    "               X_test=X_test,\n",
    "               y_test=y_test)\n",
    "data = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Disc. Layer\n"
     ]
    }
   ],
   "source": [
    "def build_mlp(Xs, disc, classnum, QUANT_UNIT):\n",
    "    rect = lasagne.nonlinearities.rectify\n",
    "    softmax = lasagne.nonlinearities.softmax\n",
    "    \n",
    "    l_in = InputLayer(shape=Xs.shape)\n",
    "    l_dense1 = DenseLayer(l_in, num_units=25, nonlinearity=rect)\n",
    "    l_dense2 = DenseLayer(l_dense1, num_units=QUANT_UNIT, nonlinearity=rect, name='param_regressor')\n",
    "    if disc:\n",
    "        sharedBins = theano.shared(None, name='sharedBins')\n",
    "        l_dis = DiscreteLayer(l_dense2, sharedBins=sharedBins, name='disclayer')\n",
    "        print(\"Using Discret. Layer\")\n",
    "    else:\n",
    "        l_dis = l_dense2\n",
    "        print(\"No Disc. Layer\")\n",
    "    l_class = DenseLayer(l_dis, num_units=classnum, nonlinearity=softmax)\n",
    "    \n",
    "    if disc:\n",
    "        return l_class, sharedBins\n",
    "    else:\n",
    "        return l_class\n",
    "\n",
    "if DISC:\n",
    "    model, sharedBins = build_mlp(data['X_train'], disc=DISC, classnum=NUM_CLASSES, QUANT_UNIT=QUANT_UNIT)\n",
    "else:\n",
    "    model = build_mlp(data['X_train'], disc=DISC, classnum=NUM_CLASSES, QUANT_UNIT=QUANT_UNIT)\n",
    "model_params = lasagne.layers.get_all_params(model, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input: dist, dist.shape = (-1, num_units)\n",
    "Find quantization bins of a given dist history\n",
    "Returns a list of (x, num_units), where x's length is a random variable\n",
    "\"\"\"\n",
    "def find_quantization_bins(dist, sharedBins):\n",
    "    # Quantizer function\n",
    "    def Q(x, y):\n",
    "        return y * np.floor((x/y) + .5)\n",
    "    \n",
    "    shape = dist.shape\n",
    "    init_Q = QUANT\n",
    "    final_Q = []\n",
    "    \n",
    "    # Theta iterator\n",
    "    for i in range(shape[1]):\n",
    "        theta_i = dist[:, i]\n",
    "        \n",
    "        # Whats is the error threshold for this distribution\n",
    "        Q_eps = np.var(theta_i) / VARIANCE_DEVIDER\n",
    "        \n",
    "        # Batch Iterator\n",
    "        final_Q_i = []\n",
    "        for j in range(shape[0]):\n",
    "            theta = theta_i[j]\n",
    "            \n",
    "            # Quantized theta = Quantization bins\n",
    "            q = init_Q[i]\n",
    "            x_i = theta\n",
    "            x_o = Q(x_i, q)\n",
    "            \n",
    "            # Optimize x_o\n",
    "\n",
    "            while(np.abs(x_o - x_i) > Q_eps):\n",
    "                q = q / 2\n",
    "                x_o = Q(x_i, q)\n",
    "            \n",
    "            # End of optimisation\n",
    "            final_Q_i.append(x_o)\n",
    "        \n",
    "        # Append to outer list\n",
    "        uniques = np.unique(np.array(final_Q_i))\n",
    "        final_Q.append(uniques.astype(theano.config.floatX))\n",
    "        \n",
    "    # Report\n",
    "    print \"New Bin Sizes: [\" + \", \".join([str(final_Q[x].shape[0]) for x in range(shape[1])] ) + \"]\"\n",
    "    sharedBins.set_value(final_Q)\n",
    "    return final_Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_functions():\n",
    "    X = T.matrix(dtype=theano.config.floatX)\n",
    "    y = T.ivector()\n",
    "\n",
    "    ## Layer History\n",
    "    if DISC:\n",
    "        l_disc = next(l for l in lasagne.layers.get_all_layers(model) if l.name is 'disclayer')\n",
    "        l_paramreg = next(l for l in lasagne.layers.get_all_layers(model) if l.name is 'param_regressor')\n",
    "        l_disc_output, l_paramreg_output = lasagne.layers.get_output([l_disc, l_paramreg], X, deterministic=False)\n",
    "    ## Layer History\n",
    "\n",
    "    # training output\n",
    "    output_train = lasagne.layers.get_output(model, X, deterministic=False)\n",
    "\n",
    "    # evaluation output. Also includes output of transform for plotting\n",
    "    output_eval = lasagne.layers.get_output(model, X, deterministic=True)\n",
    "\n",
    "    sh_lr = theano.shared(lasagne.utils.floatX(LEARNING_RATE))\n",
    "    cost = T.mean(T.nnet.categorical_crossentropy(output_train, y))\n",
    "    updates = lasagne.updates.adam(cost, model_params, learning_rate=sh_lr)\n",
    "\n",
    "    if DISC:\n",
    "        train = theano.function([X, y], [cost, output_train, l_disc_output, l_paramreg_output], updates=updates, allow_input_downcast=True)\n",
    "    else:\n",
    "        train = theano.function([X, y], [cost, output_train], updates=updates, allow_input_downcast=True)\n",
    "    eval = theano.function([X], [output_eval], allow_input_downcast=True)\n",
    "    \n",
    "    return train, eval, sh_lr\n",
    "\n",
    "train, eval, sh_lr = build_functions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_epoch(X, y):\n",
    "    # History Keeping\n",
    "    param_output = []\n",
    "    disc_output = []\n",
    "    # History\n",
    "    num_samples = X.shape[0]\n",
    "    num_batches = int(np.ceil(num_samples / float(BATCH_SIZE)))\n",
    "    costs = []\n",
    "    correct = 0\n",
    "    for i in range(num_batches):\n",
    "        idx = range(i*BATCH_SIZE, np.minimum((i+1)*BATCH_SIZE, num_samples))\n",
    "        X_batch = X[idx]\n",
    "        y_batch = y[idx]\n",
    "        if DISC:\n",
    "            cost, output_train, l_disc_output, l_paramreg_output = train(X_batch, y_batch)\n",
    "            param_output = np.append(param_output, l_paramreg_output)\n",
    "            disc_output = np.append(disc_output, l_disc_output)\n",
    "        else:\n",
    "            cost, output_train = train(X_batch, y_batch)\n",
    "        costs += [cost]\n",
    "        preds = np.argmax(output_train, axis=-1)\n",
    "        correct += np.sum(y_batch == preds)\n",
    "    \n",
    "    return np.mean(costs), correct / float(num_samples), param_output, disc_output\n",
    "\n",
    "\n",
    "def eval_epoch(X, y):\n",
    "    output_eval = eval(X)\n",
    "    preds = np.argmax(output_eval, axis=-1)\n",
    "    acc = np.mean(preds == y)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: T.cost 28.552423, Train acc 0.628571, test acc 0.622807, took 0.0347 sec.\n",
      "Epoch 1: T.cost 10.840532, Train acc 0.626374, test acc 0.263158, took 0.00213 sec.\n",
      "Epoch 2: T.cost 3.367951, Train acc 0.338462, test acc 0.377193, took 0.00197 sec.\n",
      "Epoch 3: T.cost 3.755162, Train acc 0.371429, test acc 0.377193, took 0.00198 sec.\n",
      "Epoch 4: T.cost 1.685220, Train acc 0.371429, test acc 0.640351, took 0.00127 sec.\n",
      "Epoch 5: T.cost 1.343052, Train acc 0.650549, test acc 0.622807, took 0.0015 sec.\n",
      "Epoch 6: T.cost 1.888237, Train acc 0.632967, test acc 0.631579, took 0.00121 sec.\n",
      "Epoch 7: T.cost 0.740564, Train acc 0.729670, test acc 0.394737, took 0.00122 sec.\n",
      "Epoch 8: T.cost 0.790917, Train acc 0.402198, test acc 0.377193, took 0.00124 sec.\n",
      "Epoch 9: T.cost 1.119809, Train acc 0.371429, test acc 0.377193, took 0.00123 sec.\n",
      "Epoch 10: T.cost 0.793753, Train acc 0.395604, test acc 0.938596, took 0.00125 sec.\n",
      "Epoch 11: T.cost 0.441946, Train acc 0.868132, test acc 0.798246, took 0.00124 sec.\n",
      "Epoch 12: T.cost 0.614944, Train acc 0.723077, test acc 0.710526, took 0.0012 sec.\n",
      "Epoch 13: T.cost 0.605333, Train acc 0.716484, test acc 0.885965, took 0.00121 sec.\n",
      "Epoch 14: T.cost 0.383574, Train acc 0.863736, test acc 0.947368, took 0.00124 sec.\n",
      "Epoch 15: T.cost 0.425299, Train acc 0.854945, test acc 0.763158, took 0.00122 sec.\n",
      "Epoch 16: T.cost 0.496265, Train acc 0.738462, test acc 0.894737, took 0.00121 sec.\n",
      "Epoch 17: T.cost 0.419945, Train acc 0.837363, test acc 0.947368, took 0.00124 sec.\n",
      "Epoch 18: T.cost 0.357284, Train acc 0.892308, test acc 0.894737, took 0.00122 sec.\n",
      "New LR: 0.00494999988936\n",
      "Epoch 19: T.cost 0.393831, Train acc 0.841758, test acc 0.894737, took 0.00137 sec.\n",
      "Epoch 20: T.cost 0.383584, Train acc 0.839560, test acc 0.912281, took 0.00124 sec.\n",
      "Epoch 21: T.cost 0.329106, Train acc 0.894505, test acc 0.938596, took 0.00122 sec.\n",
      "Epoch 22: T.cost 0.332296, Train acc 0.907692, test acc 0.947368, took 0.00122 sec.\n",
      "Epoch 23: T.cost 0.349809, Train acc 0.896703, test acc 0.947368, took 0.00122 sec.\n",
      "Epoch 24: T.cost 0.330392, Train acc 0.903297, test acc 0.947368, took 0.00121 sec.\n",
      "Epoch 25: T.cost 0.314360, Train acc 0.903297, test acc 0.912281, took 0.00122 sec.\n",
      "Epoch 26: T.cost 0.319602, Train acc 0.887912, test acc 0.912281, took 0.00122 sec.\n",
      "Epoch 27: T.cost 0.311283, Train acc 0.885714, test acc 0.929825, took 0.00119 sec.\n",
      "Epoch 28: T.cost 0.295427, Train acc 0.903297, test acc 0.938596, took 0.0012 sec.\n",
      "Epoch 29: T.cost 0.296324, Train acc 0.914286, test acc 0.956140, took 0.00121 sec.\n",
      "Epoch 30: T.cost 0.298746, Train acc 0.909890, test acc 0.938596, took 0.00122 sec.\n",
      "Epoch 31: T.cost 0.291695, Train acc 0.909890, test acc 0.947368, took 0.00122 sec.\n",
      "Epoch 32: T.cost 0.287859, Train acc 0.898901, test acc 0.929825, took 0.00122 sec.\n",
      "Epoch 33: T.cost 0.286372, Train acc 0.901099, test acc 0.938596, took 0.00122 sec.\n",
      "Epoch 34: T.cost 0.279977, Train acc 0.898901, test acc 0.947368, took 0.00121 sec.\n",
      "Epoch 35: T.cost 0.275992, Train acc 0.909890, test acc 0.938596, took 0.00121 sec.\n",
      "Epoch 36: T.cost 0.276680, Train acc 0.909890, test acc 0.938596, took 0.00122 sec.\n",
      "Epoch 37: T.cost 0.275267, Train acc 0.909890, test acc 0.956140, took 0.00122 sec.\n",
      "Epoch 38: T.cost 0.272590, Train acc 0.912088, test acc 0.947368, took 0.0012 sec.\n",
      "New LR: 0.00490049997345\n",
      "Epoch 39: T.cost 0.270853, Train acc 0.903297, test acc 0.938596, took 0.00133 sec.\n",
      "Epoch 40: T.cost 0.267876, Train acc 0.905495, test acc 0.947368, took 0.00121 sec.\n",
      "Epoch 41: T.cost 0.265210, Train acc 0.909890, test acc 0.938596, took 0.0012 sec.\n",
      "Epoch 42: T.cost 0.264722, Train acc 0.912088, test acc 0.938596, took 0.0012 sec.\n",
      "Epoch 43: T.cost 0.264171, Train acc 0.912088, test acc 0.947368, took 0.00119 sec.\n",
      "Epoch 44: T.cost 0.262813, Train acc 0.912088, test acc 0.947368, took 0.0012 sec.\n",
      "Epoch 45: T.cost 0.261407, Train acc 0.912088, test acc 0.947368, took 0.00121 sec.\n",
      "Epoch 46: T.cost 0.259568, Train acc 0.909890, test acc 0.956140, took 0.0012 sec.\n",
      "Epoch 47: T.cost 0.258013, Train acc 0.914286, test acc 0.938596, took 0.00121 sec.\n",
      "Epoch 48: T.cost 0.257449, Train acc 0.912088, test acc 0.947368, took 0.0012 sec.\n",
      "Epoch 49: T.cost 0.256968, Train acc 0.912088, test acc 0.947368, took 0.0012 sec.\n",
      "Epoch 50: T.cost 0.256110, Train acc 0.912088, test acc 0.956140, took 0.0012 sec.\n",
      "Epoch 51: T.cost 0.255029, Train acc 0.914286, test acc 0.956140, took 0.0012 sec.\n",
      "Epoch 52: T.cost 0.253784, Train acc 0.914286, test acc 0.956140, took 0.00122 sec.\n",
      "Epoch 53: T.cost 0.252862, Train acc 0.912088, test acc 0.947368, took 0.0012 sec.\n",
      "Epoch 54: T.cost 0.252404, Train acc 0.912088, test acc 0.947368, took 0.00119 sec.\n",
      "Epoch 55: T.cost 0.251930, Train acc 0.912088, test acc 0.947368, took 0.00123 sec.\n",
      "Epoch 56: T.cost 0.251239, Train acc 0.912088, test acc 0.956140, took 0.0012 sec.\n",
      "Epoch 57: T.cost 0.250374, Train acc 0.912088, test acc 0.956140, took 0.00119 sec.\n",
      "Epoch 58: T.cost 0.249525, Train acc 0.912088, test acc 0.947368, took 0.0012 sec.\n",
      "New LR: 0.00485149517655\n",
      "Epoch 59: T.cost 0.248949, Train acc 0.912088, test acc 0.947368, took 0.00129 sec.\n",
      "Epoch 60: T.cost 0.248536, Train acc 0.912088, test acc 0.947368, took 0.0012 sec.\n",
      "Epoch 61: T.cost 0.248060, Train acc 0.912088, test acc 0.947368, took 0.00121 sec.\n",
      "Epoch 62: T.cost 0.247441, Train acc 0.912088, test acc 0.956140, took 0.00119 sec.\n",
      "Epoch 63: T.cost 0.246756, Train acc 0.912088, test acc 0.947368, took 0.0012 sec.\n",
      "Epoch 64: T.cost 0.246172, Train acc 0.912088, test acc 0.947368, took 0.00118 sec.\n",
      "Epoch 65: T.cost 0.245744, Train acc 0.912088, test acc 0.947368, took 0.00121 sec.\n",
      "Epoch 66: T.cost 0.245343, Train acc 0.912088, test acc 0.947368, took 0.00119 sec.\n",
      "Epoch 67: T.cost 0.244857, Train acc 0.912088, test acc 0.947368, took 0.00119 sec.\n",
      "Epoch 68: T.cost 0.244297, Train acc 0.912088, test acc 0.947368, took 0.0012 sec.\n",
      "Epoch 69: T.cost 0.243756, Train acc 0.914286, test acc 0.947368, took 0.00119 sec.\n",
      "Epoch 70: T.cost 0.243312, Train acc 0.912088, test acc 0.947368, took 0.00119 sec.\n",
      "Epoch 71: T.cost 0.242921, Train acc 0.909890, test acc 0.947368, took 0.0012 sec.\n",
      "Epoch 72: T.cost 0.242495, Train acc 0.912088, test acc 0.947368, took 0.00119 sec.\n",
      "Epoch 73: T.cost 0.242012, Train acc 0.912088, test acc 0.956140, took 0.00118 sec.\n",
      "Epoch 74: T.cost 0.241518, Train acc 0.912088, test acc 0.956140, took 0.0012 sec.\n",
      "Epoch 75: T.cost 0.241079, Train acc 0.909890, test acc 0.956140, took 0.00119 sec.\n",
      "Epoch 76: T.cost 0.240688, Train acc 0.909890, test acc 0.956140, took 0.00118 sec.\n",
      "Epoch 77: T.cost 0.240285, Train acc 0.909890, test acc 0.956140, took 0.00119 sec.\n",
      "Epoch 78: T.cost 0.239841, Train acc 0.909890, test acc 0.956140, took 0.0012 sec.\n",
      "New LR: 0.00480298042763\n",
      "Epoch 79: T.cost 0.239383, Train acc 0.909890, test acc 0.956140, took 0.0013 sec.\n",
      "Epoch 80: T.cost 0.238945, Train acc 0.909890, test acc 0.956140, took 0.0012 sec.\n",
      "Epoch 81: T.cost 0.238556, Train acc 0.912088, test acc 0.956140, took 0.00121 sec.\n",
      "Epoch 82: T.cost 0.238160, Train acc 0.912088, test acc 0.956140, took 0.00119 sec.\n",
      "Epoch 83: T.cost 0.237739, Train acc 0.912088, test acc 0.956140, took 0.00113 sec.\n",
      "Epoch 84: T.cost 0.237308, Train acc 0.912088, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 85: T.cost 0.236895, Train acc 0.912088, test acc 0.956140, took 0.00101 sec.\n",
      "Epoch 86: T.cost 0.236503, Train acc 0.912088, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 87: T.cost 0.236109, Train acc 0.912088, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 88: T.cost 0.235698, Train acc 0.912088, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 89: T.cost 0.235281, Train acc 0.912088, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 90: T.cost 0.234874, Train acc 0.912088, test acc 0.956140, took 0.00101 sec.\n",
      "Epoch 91: T.cost 0.234479, Train acc 0.912088, test acc 0.956140, took 0.00101 sec.\n",
      "Epoch 92: T.cost 0.234082, Train acc 0.912088, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 93: T.cost 0.233677, Train acc 0.912088, test acc 0.956140, took 0.000995 sec.\n",
      "Epoch 94: T.cost 0.233268, Train acc 0.912088, test acc 0.956140, took 0.000997 sec.\n",
      "Epoch 95: T.cost 0.232865, Train acc 0.912088, test acc 0.956140, took 0.00101 sec.\n",
      "Epoch 96: T.cost 0.232468, Train acc 0.909890, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 97: T.cost 0.232070, Train acc 0.909890, test acc 0.964912, took 0.000995 sec.\n",
      "Epoch 98: T.cost 0.231666, Train acc 0.909890, test acc 0.964912, took 0.000993 sec.\n",
      "New LR: 0.00475495065562\n",
      "Epoch 99: T.cost 0.231261, Train acc 0.909890, test acc 0.964912, took 0.0011 sec.\n",
      "Epoch 100: T.cost 0.230849, Train acc 0.909890, test acc 0.964912, took 0.00101 sec.\n",
      "Epoch 101: T.cost 0.230457, Train acc 0.909890, test acc 0.964912, took 0.00101 sec.\n",
      "Epoch 102: T.cost 0.230058, Train acc 0.909890, test acc 0.964912, took 0.00099 sec.\n",
      "Epoch 103: T.cost 0.229655, Train acc 0.909890, test acc 0.964912, took 0.000998 sec.\n",
      "Epoch 104: T.cost 0.229255, Train acc 0.909890, test acc 0.964912, took 0.000992 sec.\n",
      "Epoch 105: T.cost 0.228861, Train acc 0.909890, test acc 0.964912, took 0.000987 sec.\n",
      "Epoch 106: T.cost 0.228467, Train acc 0.909890, test acc 0.964912, took 0.000991 sec.\n",
      "Epoch 107: T.cost 0.228068, Train acc 0.909890, test acc 0.964912, took 0.001 sec.\n",
      "Epoch 108: T.cost 0.227665, Train acc 0.909890, test acc 0.964912, took 0.000984 sec.\n",
      "Epoch 109: T.cost 0.227266, Train acc 0.909890, test acc 0.964912, took 0.000983 sec.\n",
      "Epoch 110: T.cost 0.226871, Train acc 0.909890, test acc 0.964912, took 0.000985 sec.\n",
      "Epoch 111: T.cost 0.226474, Train acc 0.909890, test acc 0.964912, took 0.001 sec.\n",
      "Epoch 112: T.cost 0.226073, Train acc 0.909890, test acc 0.964912, took 0.000978 sec.\n",
      "Epoch 113: T.cost 0.225671, Train acc 0.909890, test acc 0.973684, took 0.000971 sec.\n",
      "Epoch 114: T.cost 0.225272, Train acc 0.909890, test acc 0.973684, took 0.000978 sec.\n",
      "Epoch 115: T.cost 0.224875, Train acc 0.909890, test acc 0.973684, took 0.000986 sec.\n",
      "Epoch 116: T.cost 0.224475, Train acc 0.909890, test acc 0.973684, took 0.000976 sec.\n",
      "Epoch 117: T.cost 0.224073, Train acc 0.909890, test acc 0.973684, took 0.000974 sec.\n",
      "Epoch 118: T.cost 0.223671, Train acc 0.909890, test acc 0.973684, took 0.000972 sec.\n",
      "New LR: 0.00470740125049\n",
      "Epoch 119: T.cost 0.223272, Train acc 0.909890, test acc 0.973684, took 0.00108 sec.\n",
      "Epoch 120: T.cost 0.222859, Train acc 0.909890, test acc 0.973684, took 0.000986 sec.\n",
      "Epoch 121: T.cost 0.222464, Train acc 0.909890, test acc 0.973684, took 0.00099 sec.\n",
      "Epoch 122: T.cost 0.222061, Train acc 0.909890, test acc 0.973684, took 0.000967 sec.\n",
      "Epoch 123: T.cost 0.221658, Train acc 0.909890, test acc 0.973684, took 0.000967 sec.\n",
      "Epoch 124: T.cost 0.221263, Train acc 0.909890, test acc 0.973684, took 0.000963 sec.\n",
      "Epoch 125: T.cost 0.220869, Train acc 0.909890, test acc 0.973684, took 0.000965 sec.\n",
      "Epoch 126: T.cost 0.220469, Train acc 0.909890, test acc 0.973684, took 0.000972 sec.\n",
      "Epoch 127: T.cost 0.220066, Train acc 0.912088, test acc 0.973684, took 0.00097 sec.\n",
      "Epoch 128: T.cost 0.219665, Train acc 0.912088, test acc 0.973684, took 0.000964 sec.\n",
      "Epoch 129: T.cost 0.219269, Train acc 0.912088, test acc 0.973684, took 0.000965 sec.\n",
      "Epoch 130: T.cost 0.218870, Train acc 0.912088, test acc 0.973684, took 0.000978 sec.\n",
      "Epoch 131: T.cost 0.218467, Train acc 0.912088, test acc 0.973684, took 0.000981 sec.\n",
      "Epoch 132: T.cost 0.218064, Train acc 0.912088, test acc 0.973684, took 0.000961 sec.\n",
      "Epoch 133: T.cost 0.217664, Train acc 0.912088, test acc 0.973684, took 0.000962 sec.\n",
      "Epoch 134: T.cost 0.217264, Train acc 0.912088, test acc 0.973684, took 0.000969 sec.\n",
      "Epoch 135: T.cost 0.216863, Train acc 0.912088, test acc 0.973684, took 0.000962 sec.\n",
      "Epoch 136: T.cost 0.216459, Train acc 0.912088, test acc 0.973684, took 0.000959 sec.\n",
      "Epoch 137: T.cost 0.216056, Train acc 0.912088, test acc 0.973684, took 0.000958 sec.\n",
      "Epoch 138: T.cost 0.215655, Train acc 0.912088, test acc 0.973684, took 0.000971 sec.\n",
      "New LR: 0.00466032714117\n",
      "Epoch 139: T.cost 0.215253, Train acc 0.912088, test acc 0.973684, took 0.00105 sec.\n",
      "Epoch 140: T.cost 0.214834, Train acc 0.912088, test acc 0.973684, took 0.00096 sec.\n",
      "Epoch 141: T.cost 0.214438, Train acc 0.912088, test acc 0.973684, took 0.00096 sec.\n",
      "Epoch 142: T.cost 0.214032, Train acc 0.912088, test acc 0.973684, took 0.00097 sec.\n",
      "Epoch 143: T.cost 0.213629, Train acc 0.912088, test acc 0.973684, took 0.000958 sec.\n",
      "Epoch 144: T.cost 0.213232, Train acc 0.912088, test acc 0.973684, took 0.000959 sec.\n",
      "Epoch 145: T.cost 0.212835, Train acc 0.912088, test acc 0.973684, took 0.000959 sec.\n",
      "Epoch 146: T.cost 0.212432, Train acc 0.912088, test acc 0.973684, took 0.000968 sec.\n",
      "Epoch 147: T.cost 0.212027, Train acc 0.912088, test acc 0.973684, took 0.000958 sec.\n",
      "Epoch 148: T.cost 0.211627, Train acc 0.912088, test acc 0.973684, took 0.000958 sec.\n",
      "Epoch 149: T.cost 0.211229, Train acc 0.912088, test acc 0.973684, took 0.000961 sec.\n",
      "Epoch 150: T.cost 0.210826, Train acc 0.912088, test acc 0.973684, took 0.000972 sec.\n",
      "Epoch 151: T.cost 0.210421, Train acc 0.912088, test acc 0.973684, took 0.00096 sec.\n",
      "Epoch 152: T.cost 0.210019, Train acc 0.914286, test acc 0.973684, took 0.00105 sec.\n",
      "Epoch 153: T.cost 0.209618, Train acc 0.914286, test acc 0.973684, took 0.000959 sec.\n",
      "Epoch 154: T.cost 0.209216, Train acc 0.914286, test acc 0.973684, took 0.000972 sec.\n",
      "Epoch 155: T.cost 0.208811, Train acc 0.914286, test acc 0.973684, took 0.000959 sec.\n",
      "Epoch 156: T.cost 0.208407, Train acc 0.914286, test acc 0.973684, took 0.000958 sec.\n",
      "Epoch 157: T.cost 0.208004, Train acc 0.914286, test acc 0.973684, took 0.000958 sec.\n",
      "Epoch 158: T.cost 0.207601, Train acc 0.914286, test acc 0.973684, took 0.000964 sec.\n",
      "New LR: 0.00461372371763\n",
      "Epoch 159: T.cost 0.207196, Train acc 0.914286, test acc 0.973684, took 0.00105 sec.\n",
      "Epoch 160: T.cost 0.206774, Train acc 0.914286, test acc 0.973684, took 0.000964 sec.\n",
      "Epoch 161: T.cost 0.206379, Train acc 0.914286, test acc 0.973684, took 0.000956 sec.\n",
      "Epoch 162: T.cost 0.205970, Train acc 0.914286, test acc 0.973684, took 0.000968 sec.\n",
      "Epoch 163: T.cost 0.205564, Train acc 0.914286, test acc 0.973684, took 0.000956 sec.\n",
      "Epoch 164: T.cost 0.205169, Train acc 0.914286, test acc 0.973684, took 0.000957 sec.\n",
      "Epoch 165: T.cost 0.204772, Train acc 0.914286, test acc 0.973684, took 0.000956 sec.\n",
      "Epoch 166: T.cost 0.204366, Train acc 0.914286, test acc 0.973684, took 0.000964 sec.\n",
      "Epoch 167: T.cost 0.203960, Train acc 0.914286, test acc 0.973684, took 0.000955 sec.\n",
      "Epoch 168: T.cost 0.203562, Train acc 0.914286, test acc 0.973684, took 0.000959 sec.\n",
      "Epoch 169: T.cost 0.203163, Train acc 0.914286, test acc 0.973684, took 0.000965 sec.\n",
      "Epoch 170: T.cost 0.202758, Train acc 0.914286, test acc 0.973684, took 0.000969 sec.\n",
      "Epoch 171: T.cost 0.202353, Train acc 0.914286, test acc 0.973684, took 0.000957 sec.\n",
      "Epoch 172: T.cost 0.201953, Train acc 0.914286, test acc 0.973684, took 0.000956 sec.\n",
      "Epoch 173: T.cost 0.201553, Train acc 0.914286, test acc 0.973684, took 0.000958 sec.\n",
      "Epoch 174: T.cost 0.201148, Train acc 0.916484, test acc 0.973684, took 0.000972 sec.\n",
      "Epoch 175: T.cost 0.200744, Train acc 0.916484, test acc 0.973684, took 0.000992 sec.\n",
      "Epoch 176: T.cost 0.200344, Train acc 0.916484, test acc 0.973684, took 0.00264 sec.\n",
      "Epoch 177: T.cost 0.199942, Train acc 0.916484, test acc 0.973684, took 0.00112 sec.\n",
      "Epoch 178: T.cost 0.199538, Train acc 0.916484, test acc 0.973684, took 0.000957 sec.\n",
      "New LR: 0.00456758636981\n",
      "Epoch 179: T.cost 0.199135, Train acc 0.916484, test acc 0.973684, took 0.00105 sec.\n",
      "Epoch 180: T.cost 0.198715, Train acc 0.916484, test acc 0.973684, took 0.000983 sec.\n",
      "Epoch 181: T.cost 0.198321, Train acc 0.916484, test acc 0.973684, took 0.000961 sec.\n",
      "Epoch 182: T.cost 0.197910, Train acc 0.916484, test acc 0.973684, took 0.000955 sec.\n",
      "Epoch 183: T.cost 0.197507, Train acc 0.916484, test acc 0.973684, took 0.000956 sec.\n",
      "Epoch 184: T.cost 0.197120, Train acc 0.916484, test acc 0.973684, took 0.000968 sec.\n",
      "Epoch 185: T.cost 0.196723, Train acc 0.916484, test acc 0.973684, took 0.000953 sec.\n",
      "Epoch 186: T.cost 0.196315, Train acc 0.916484, test acc 0.973684, took 0.000955 sec.\n",
      "Epoch 187: T.cost 0.195916, Train acc 0.918681, test acc 0.973684, took 0.000956 sec.\n",
      "Epoch 188: T.cost 0.195525, Train acc 0.918681, test acc 0.973684, took 0.000972 sec.\n",
      "Epoch 189: T.cost 0.195126, Train acc 0.918681, test acc 0.973684, took 0.000958 sec.\n",
      "Epoch 190: T.cost 0.194721, Train acc 0.918681, test acc 0.973684, took 0.000952 sec.\n",
      "Epoch 191: T.cost 0.194324, Train acc 0.918681, test acc 0.973684, took 0.000952 sec.\n",
      "Epoch 192: T.cost 0.193931, Train acc 0.918681, test acc 0.973684, took 0.000963 sec.\n",
      "Epoch 193: T.cost 0.193531, Train acc 0.920879, test acc 0.973684, took 0.000962 sec.\n",
      "Epoch 194: T.cost 0.193130, Train acc 0.920879, test acc 0.964912, took 0.000954 sec.\n",
      "Epoch 195: T.cost 0.192734, Train acc 0.923077, test acc 0.964912, took 0.000953 sec.\n",
      "Epoch 196: T.cost 0.192339, Train acc 0.925275, test acc 0.964912, took 0.000967 sec.\n",
      "Epoch 197: T.cost 0.191940, Train acc 0.925275, test acc 0.964912, took 0.000955 sec.\n",
      "Epoch 198: T.cost 0.191541, Train acc 0.925275, test acc 0.964912, took 0.000954 sec.\n",
      "New LR: 0.00452191048767\n",
      "Epoch 199: T.cost 0.191146, Train acc 0.925275, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 200: T.cost 0.190730, Train acc 0.925275, test acc 0.964912, took 0.00097 sec.\n",
      "Epoch 201: T.cost 0.190340, Train acc 0.927473, test acc 0.964912, took 0.000961 sec.\n",
      "Epoch 202: T.cost 0.189931, Train acc 0.927473, test acc 0.964912, took 0.000957 sec.\n",
      "Epoch 203: T.cost 0.189540, Train acc 0.927473, test acc 0.964912, took 0.000954 sec.\n",
      "Epoch 204: T.cost 0.189162, Train acc 0.927473, test acc 0.964912, took 0.000966 sec.\n",
      "Epoch 205: T.cost 0.188766, Train acc 0.927473, test acc 0.964912, took 0.000965 sec.\n",
      "Epoch 206: T.cost 0.188366, Train acc 0.927473, test acc 0.964912, took 0.000961 sec.\n",
      "Epoch 207: T.cost 0.187981, Train acc 0.927473, test acc 0.964912, took 0.000958 sec.\n",
      "Epoch 208: T.cost 0.187597, Train acc 0.927473, test acc 0.964912, took 0.000966 sec.\n",
      "Epoch 209: T.cost 0.187200, Train acc 0.927473, test acc 0.964912, took 0.000953 sec.\n",
      "Epoch 210: T.cost 0.186808, Train acc 0.927473, test acc 0.964912, took 0.000957 sec.\n",
      "Epoch 211: T.cost 0.186425, Train acc 0.927473, test acc 0.964912, took 0.000955 sec.\n",
      "Epoch 212: T.cost 0.186037, Train acc 0.927473, test acc 0.964912, took 0.000962 sec.\n",
      "Epoch 213: T.cost 0.185643, Train acc 0.927473, test acc 0.964912, took 0.000968 sec.\n",
      "Epoch 214: T.cost 0.185256, Train acc 0.927473, test acc 0.964912, took 0.000962 sec.\n",
      "Epoch 215: T.cost 0.184872, Train acc 0.927473, test acc 0.964912, took 0.000954 sec.\n",
      "Epoch 216: T.cost 0.184483, Train acc 0.927473, test acc 0.964912, took 0.000966 sec.\n",
      "Epoch 217: T.cost 0.184094, Train acc 0.927473, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 218: T.cost 0.183710, Train acc 0.927473, test acc 0.964912, took 0.00103 sec.\n",
      "New LR: 0.00447669146117\n",
      "Epoch 219: T.cost 0.183326, Train acc 0.929670, test acc 0.964912, took 0.00112 sec.\n",
      "Epoch 220: T.cost 0.182913, Train acc 0.929670, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 221: T.cost 0.182537, Train acc 0.929670, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 222: T.cost 0.182139, Train acc 0.929670, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 223: T.cost 0.181761, Train acc 0.929670, test acc 0.964912, took 0.00102 sec.\n",
      "Epoch 224: T.cost 0.181395, Train acc 0.929670, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 225: T.cost 0.181008, Train acc 0.929670, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 226: T.cost 0.180622, Train acc 0.929670, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 227: T.cost 0.180254, Train acc 0.929670, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 228: T.cost 0.179879, Train acc 0.929670, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 229: T.cost 0.179493, Train acc 0.929670, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 230: T.cost 0.179119, Train acc 0.929670, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 231: T.cost 0.178750, Train acc 0.929670, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 232: T.cost 0.178370, Train acc 0.931868, test acc 0.964912, took 0.00102 sec.\n",
      "Epoch 233: T.cost 0.177993, Train acc 0.931868, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 234: T.cost 0.177625, Train acc 0.931868, test acc 0.964912, took 0.00102 sec.\n",
      "Epoch 235: T.cost 0.177251, Train acc 0.931868, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 236: T.cost 0.176875, Train acc 0.934066, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 237: T.cost 0.176506, Train acc 0.934066, test acc 0.964912, took 0.00102 sec.\n",
      "Epoch 238: T.cost 0.176136, Train acc 0.934066, test acc 0.964912, took 0.00102 sec.\n",
      "New LR: 0.00443192468025\n",
      "Epoch 239: T.cost 0.175763, Train acc 0.934066, test acc 0.964912, took 0.00112 sec.\n",
      "Epoch 240: T.cost 0.175365, Train acc 0.934066, test acc 0.964912, took 0.00102 sec.\n",
      "Epoch 241: T.cost 0.175007, Train acc 0.934066, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 242: T.cost 0.174615, Train acc 0.934066, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 243: T.cost 0.174261, Train acc 0.934066, test acc 0.956140, took 0.00101 sec.\n",
      "Epoch 244: T.cost 0.173913, Train acc 0.934066, test acc 0.956140, took 0.00101 sec.\n",
      "Epoch 245: T.cost 0.173533, Train acc 0.934066, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 246: T.cost 0.173168, Train acc 0.934066, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 247: T.cost 0.172822, Train acc 0.934066, test acc 0.956140, took 0.00101 sec.\n",
      "Epoch 248: T.cost 0.172454, Train acc 0.934066, test acc 0.956140, took 0.00101 sec.\n",
      "Epoch 249: T.cost 0.172085, Train acc 0.934066, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 250: T.cost 0.171738, Train acc 0.934066, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 251: T.cost 0.171378, Train acc 0.934066, test acc 0.956140, took 0.00101 sec.\n",
      "Epoch 252: T.cost 0.171011, Train acc 0.931868, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 253: T.cost 0.170660, Train acc 0.931868, test acc 0.956140, took 0.00101 sec.\n",
      "Epoch 254: T.cost 0.170307, Train acc 0.931868, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 255: T.cost 0.169944, Train acc 0.931868, test acc 0.956140, took 0.00101 sec.\n",
      "Epoch 256: T.cost 0.169591, Train acc 0.931868, test acc 0.956140, took 0.00101 sec.\n",
      "Epoch 257: T.cost 0.169241, Train acc 0.931868, test acc 0.956140, took 0.001 sec.\n",
      "Epoch 258: T.cost 0.168882, Train acc 0.931868, test acc 0.956140, took 0.00102 sec.\n",
      "New LR: 0.00438760553487\n",
      "Epoch 259: T.cost 0.168529, Train acc 0.931868, test acc 0.956140, took 0.00109 sec.\n",
      "Epoch 260: T.cost 0.168149, Train acc 0.931868, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 261: T.cost 0.167802, Train acc 0.931868, test acc 0.956140, took 0.00101 sec.\n",
      "Epoch 262: T.cost 0.167424, Train acc 0.931868, test acc 0.956140, took 0.00101 sec.\n",
      "Epoch 263: T.cost 0.167099, Train acc 0.931868, test acc 0.956140, took 0.00101 sec.\n",
      "Epoch 264: T.cost 0.166761, Train acc 0.931868, test acc 0.956140, took 0.00101 sec.\n",
      "Epoch 265: T.cost 0.166391, Train acc 0.934066, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 266: T.cost 0.166058, Train acc 0.934066, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 267: T.cost 0.165724, Train acc 0.934066, test acc 0.964912, took 0.00101 sec.\n",
      "Epoch 268: T.cost 0.165363, Train acc 0.934066, test acc 0.964912, took 0.00101 sec.\n",
      "Epoch 269: T.cost 0.165026, Train acc 0.934066, test acc 0.964912, took 0.00102 sec.\n",
      "Epoch 270: T.cost 0.164693, Train acc 0.934066, test acc 0.964912, took 0.00101 sec.\n",
      "Epoch 271: T.cost 0.164339, Train acc 0.934066, test acc 0.964912, took 0.00101 sec.\n",
      "Epoch 272: T.cost 0.164001, Train acc 0.934066, test acc 0.964912, took 0.00102 sec.\n",
      "Epoch 273: T.cost 0.163669, Train acc 0.934066, test acc 0.964912, took 0.00102 sec.\n",
      "Epoch 274: T.cost 0.163320, Train acc 0.934066, test acc 0.964912, took 0.00101 sec.\n",
      "Epoch 275: T.cost 0.162982, Train acc 0.934066, test acc 0.964912, took 0.00101 sec.\n",
      "Epoch 276: T.cost 0.162651, Train acc 0.934066, test acc 0.964912, took 0.00101 sec.\n",
      "Epoch 277: T.cost 0.162306, Train acc 0.934066, test acc 0.964912, took 0.00101 sec.\n",
      "Epoch 278: T.cost 0.161969, Train acc 0.934066, test acc 0.964912, took 0.00103 sec.\n",
      "New LR: 0.00434372941498\n",
      "Epoch 279: T.cost 0.161638, Train acc 0.934066, test acc 0.964912, took 0.00109 sec.\n",
      "Epoch 280: T.cost 0.161263, Train acc 0.934066, test acc 0.964912, took 0.00102 sec.\n",
      "Epoch 281: T.cost 0.160933, Train acc 0.934066, test acc 0.964912, took 0.00101 sec.\n",
      "Epoch 282: T.cost 0.160571, Train acc 0.934066, test acc 0.964912, took 0.00102 sec.\n",
      "Epoch 283: T.cost 0.160266, Train acc 0.934066, test acc 0.964912, took 0.00102 sec.\n",
      "Epoch 284: T.cost 0.159937, Train acc 0.934066, test acc 0.964912, took 0.00106 sec.\n",
      "Epoch 285: T.cost 0.159585, Train acc 0.934066, test acc 0.964912, took 0.00101 sec.\n",
      "Epoch 286: T.cost 0.159275, Train acc 0.934066, test acc 0.964912, took 0.001 sec.\n",
      "Epoch 287: T.cost 0.158946, Train acc 0.934066, test acc 0.964912, took 0.00101 sec.\n",
      "Epoch 288: T.cost 0.158602, Train acc 0.934066, test acc 0.964912, took 0.00102 sec.\n",
      "Epoch 289: T.cost 0.158290, Train acc 0.934066, test acc 0.964912, took 0.001 sec.\n",
      "Epoch 290: T.cost 0.157961, Train acc 0.936264, test acc 0.964912, took 0.00101 sec.\n",
      "Epoch 291: T.cost 0.157624, Train acc 0.936264, test acc 0.964912, took 0.001 sec.\n",
      "Epoch 292: T.cost 0.157309, Train acc 0.936264, test acc 0.964912, took 0.00102 sec.\n",
      "Epoch 293: T.cost 0.156981, Train acc 0.936264, test acc 0.964912, took 0.00102 sec.\n",
      "Epoch 294: T.cost 0.156649, Train acc 0.936264, test acc 0.964912, took 0.00102 sec.\n",
      "Epoch 295: T.cost 0.156334, Train acc 0.936264, test acc 0.964912, took 0.001 sec.\n",
      "Epoch 296: T.cost 0.156006, Train acc 0.936264, test acc 0.964912, took 0.001 sec.\n",
      "Epoch 297: T.cost 0.155679, Train acc 0.936264, test acc 0.964912, took 0.001 sec.\n",
      "Epoch 298: T.cost 0.155363, Train acc 0.938462, test acc 0.964912, took 0.00102 sec.\n",
      "New LR: 0.00430029217154\n",
      "Epoch 299: T.cost 0.155035, Train acc 0.938462, test acc 0.964912, took 0.0011 sec.\n",
      "Epoch 300: T.cost 0.154675, Train acc 0.938462, test acc 0.964912, took 0.00101 sec.\n",
      "Epoch 301: T.cost 0.154359, Train acc 0.938462, test acc 0.964912, took 0.00101 sec.\n",
      "Epoch 302: T.cost 0.154000, Train acc 0.938462, test acc 0.964912, took 0.00101 sec.\n",
      "Epoch 303: T.cost 0.153723, Train acc 0.938462, test acc 0.964912, took 0.00102 sec.\n",
      "Epoch 304: T.cost 0.153395, Train acc 0.938462, test acc 0.964912, took 0.00101 sec.\n",
      "Epoch 305: T.cost 0.153056, Train acc 0.938462, test acc 0.964912, took 0.00101 sec.\n",
      "Epoch 306: T.cost 0.152772, Train acc 0.938462, test acc 0.964912, took 0.001 sec.\n",
      "Epoch 307: T.cost 0.152438, Train acc 0.938462, test acc 0.964912, took 0.00101 sec.\n",
      "Epoch 308: T.cost 0.152115, Train acc 0.938462, test acc 0.964912, took 0.00101 sec.\n",
      "Epoch 309: T.cost 0.151822, Train acc 0.938462, test acc 0.964912, took 0.001 sec.\n",
      "Epoch 310: T.cost 0.151488, Train acc 0.938462, test acc 0.964912, took 0.001 sec.\n",
      "Epoch 311: T.cost 0.151176, Train acc 0.938462, test acc 0.964912, took 0.00102 sec.\n",
      "Epoch 312: T.cost 0.150874, Train acc 0.938462, test acc 0.964912, took 0.00101 sec.\n",
      "Epoch 313: T.cost 0.150544, Train acc 0.938462, test acc 0.964912, took 0.00101 sec.\n",
      "Epoch 314: T.cost 0.150239, Train acc 0.938462, test acc 0.964912, took 0.000999 sec.\n",
      "Epoch 315: T.cost 0.149929, Train acc 0.938462, test acc 0.964912, took 0.00101 sec.\n",
      "Epoch 316: T.cost 0.149606, Train acc 0.938462, test acc 0.964912, took 0.001 sec.\n",
      "Epoch 317: T.cost 0.149310, Train acc 0.938462, test acc 0.964912, took 0.001 sec.\n",
      "Epoch 318: T.cost 0.148991, Train acc 0.938462, test acc 0.964912, took 0.001 sec.\n",
      "New LR: 0.0042572891945\n",
      "Epoch 319: T.cost 0.148678, Train acc 0.938462, test acc 0.964912, took 0.0011 sec.\n",
      "Epoch 320: T.cost 0.148338, Train acc 0.938462, test acc 0.964912, took 0.00101 sec.\n",
      "Epoch 321: T.cost 0.148016, Train acc 0.938462, test acc 0.964912, took 0.001 sec.\n",
      "Epoch 322: T.cost 0.147676, Train acc 0.938462, test acc 0.964912, took 0.00101 sec.\n",
      "Epoch 323: T.cost 0.147424, Train acc 0.938462, test acc 0.964912, took 0.001 sec.\n",
      "Epoch 324: T.cost 0.147081, Train acc 0.938462, test acc 0.964912, took 0.00101 sec.\n",
      "Epoch 325: T.cost 0.146779, Train acc 0.938462, test acc 0.964912, took 0.00101 sec.\n",
      "Epoch 326: T.cost 0.146502, Train acc 0.938462, test acc 0.964912, took 0.00102 sec.\n",
      "Epoch 327: T.cost 0.146162, Train acc 0.938462, test acc 0.964912, took 0.000998 sec.\n",
      "Epoch 328: T.cost 0.145881, Train acc 0.938462, test acc 0.964912, took 0.00101 sec.\n",
      "Epoch 329: T.cost 0.145579, Train acc 0.938462, test acc 0.964912, took 0.00101 sec.\n",
      "Epoch 330: T.cost 0.145256, Train acc 0.938462, test acc 0.964912, took 0.00101 sec.\n",
      "Epoch 331: T.cost 0.144978, Train acc 0.938462, test acc 0.964912, took 0.001 sec.\n",
      "Epoch 332: T.cost 0.144661, Train acc 0.938462, test acc 0.964912, took 0.00101 sec.\n",
      "Epoch 333: T.cost 0.144358, Train acc 0.938462, test acc 0.964912, took 0.00101 sec.\n",
      "Epoch 334: T.cost 0.144070, Train acc 0.938462, test acc 0.964912, took 0.00101 sec.\n",
      "Epoch 335: T.cost 0.143753, Train acc 0.938462, test acc 0.964912, took 0.00101 sec.\n",
      "Epoch 336: T.cost 0.143462, Train acc 0.938462, test acc 0.964912, took 0.001 sec.\n",
      "Epoch 337: T.cost 0.143161, Train acc 0.938462, test acc 0.964912, took 0.00101 sec.\n",
      "Epoch 338: T.cost 0.142854, Train acc 0.938462, test acc 0.964912, took 0.00101 sec.\n",
      "New LR: 0.00421471633483\n",
      "Epoch 339: T.cost 0.142565, Train acc 0.938462, test acc 0.964912, took 0.0011 sec.\n",
      "Epoch 340: T.cost 0.142215, Train acc 0.938462, test acc 0.964912, took 0.00101 sec.\n",
      "Epoch 341: T.cost 0.141909, Train acc 0.938462, test acc 0.964912, took 0.00101 sec.\n",
      "Epoch 342: T.cost 0.141581, Train acc 0.938462, test acc 0.964912, took 0.001 sec.\n",
      "Epoch 343: T.cost 0.141337, Train acc 0.938462, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 344: T.cost 0.140999, Train acc 0.938462, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 345: T.cost 0.140721, Train acc 0.938462, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 346: T.cost 0.140439, Train acc 0.938462, test acc 0.964912, took 0.00101 sec.\n",
      "Epoch 347: T.cost 0.140115, Train acc 0.938462, test acc 0.964912, took 0.00101 sec.\n",
      "Epoch 348: T.cost 0.139853, Train acc 0.938462, test acc 0.964912, took 0.00102 sec.\n",
      "Epoch 349: T.cost 0.139542, Train acc 0.938462, test acc 0.964912, took 0.00102 sec.\n",
      "Epoch 350: T.cost 0.139250, Train acc 0.938462, test acc 0.964912, took 0.00102 sec.\n",
      "Epoch 351: T.cost 0.138972, Train acc 0.938462, test acc 0.964912, took 0.00101 sec.\n",
      "Epoch 352: T.cost 0.138661, Train acc 0.938462, test acc 0.964912, took 0.00101 sec.\n",
      "Epoch 353: T.cost 0.138386, Train acc 0.938462, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 354: T.cost 0.138088, Train acc 0.938462, test acc 0.964912, took 0.00102 sec.\n",
      "Epoch 355: T.cost 0.137795, Train acc 0.938462, test acc 0.964912, took 0.00101 sec.\n",
      "Epoch 356: T.cost 0.137516, Train acc 0.938462, test acc 0.964912, took 0.00101 sec.\n",
      "Epoch 357: T.cost 0.137214, Train acc 0.938462, test acc 0.964912, took 0.00101 sec.\n",
      "Epoch 358: T.cost 0.136935, Train acc 0.938462, test acc 0.964912, took 0.00101 sec.\n",
      "New LR: 0.00417256898247\n",
      "Epoch 359: T.cost 0.136643, Train acc 0.938462, test acc 0.973684, took 0.0011 sec.\n",
      "Epoch 360: T.cost 0.136307, Train acc 0.938462, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 361: T.cost 0.136011, Train acc 0.938462, test acc 0.964912, took 0.00101 sec.\n",
      "Epoch 362: T.cost 0.135688, Train acc 0.938462, test acc 0.973684, took 0.00101 sec.\n",
      "Epoch 363: T.cost 0.135471, Train acc 0.938462, test acc 0.964912, took 0.00101 sec.\n",
      "Epoch 364: T.cost 0.135122, Train acc 0.938462, test acc 0.973684, took 0.00103 sec.\n",
      "Epoch 365: T.cost 0.134883, Train acc 0.938462, test acc 0.973684, took 0.00101 sec.\n",
      "Epoch 366: T.cost 0.134591, Train acc 0.938462, test acc 0.973684, took 0.00101 sec.\n",
      "Epoch 367: T.cost 0.134289, Train acc 0.938462, test acc 0.973684, took 0.00102 sec.\n",
      "Epoch 368: T.cost 0.134047, Train acc 0.938462, test acc 0.973684, took 0.00233 sec.\n",
      "Epoch 369: T.cost 0.133726, Train acc 0.938462, test acc 0.973684, took 0.00157 sec.\n",
      "Epoch 370: T.cost 0.133477, Train acc 0.938462, test acc 0.973684, took 0.00106 sec.\n",
      "Epoch 371: T.cost 0.133186, Train acc 0.938462, test acc 0.973684, took 0.00103 sec.\n",
      "Epoch 372: T.cost 0.132902, Train acc 0.938462, test acc 0.973684, took 0.00106 sec.\n",
      "Epoch 373: T.cost 0.132643, Train acc 0.938462, test acc 0.973684, took 0.000999 sec.\n",
      "Epoch 374: T.cost 0.132343, Train acc 0.938462, test acc 0.973684, took 0.000982 sec.\n",
      "Epoch 375: T.cost 0.132088, Train acc 0.938462, test acc 0.973684, took 0.00098 sec.\n",
      "Epoch 376: T.cost 0.131799, Train acc 0.938462, test acc 0.973684, took 0.000982 sec.\n",
      "Epoch 377: T.cost 0.131527, Train acc 0.938462, test acc 0.973684, took 0.000982 sec.\n",
      "Epoch 378: T.cost 0.131259, Train acc 0.938462, test acc 0.973684, took 0.000995 sec.\n",
      "New LR: 0.00413084344938\n",
      "Epoch 379: T.cost 0.130975, Train acc 0.938462, test acc 0.973684, took 0.00107 sec.\n",
      "Epoch 380: T.cost 0.130667, Train acc 0.938462, test acc 0.973684, took 0.000991 sec.\n",
      "Epoch 381: T.cost 0.130359, Train acc 0.938462, test acc 0.973684, took 0.000995 sec.\n",
      "Epoch 382: T.cost 0.130074, Train acc 0.938462, test acc 0.973684, took 0.000984 sec.\n",
      "Epoch 383: T.cost 0.129863, Train acc 0.938462, test acc 0.973684, took 0.000983 sec.\n",
      "Epoch 384: T.cost 0.129520, Train acc 0.938462, test acc 0.973684, took 0.000986 sec.\n",
      "Epoch 385: T.cost 0.129327, Train acc 0.938462, test acc 0.973684, took 0.000997 sec.\n",
      "Epoch 386: T.cost 0.129007, Train acc 0.938462, test acc 0.973684, took 0.000992 sec.\n",
      "Epoch 387: T.cost 0.128769, Train acc 0.938462, test acc 0.973684, took 0.000986 sec.\n",
      "Epoch 388: T.cost 0.128508, Train acc 0.938462, test acc 0.973684, took 0.000989 sec.\n",
      "Epoch 389: T.cost 0.128220, Train acc 0.938462, test acc 0.973684, took 0.000998 sec.\n",
      "Epoch 390: T.cost 0.127996, Train acc 0.938462, test acc 0.973684, took 0.000993 sec.\n",
      "Epoch 391: T.cost 0.127694, Train acc 0.938462, test acc 0.973684, took 0.000994 sec.\n",
      "Epoch 392: T.cost 0.127469, Train acc 0.936264, test acc 0.973684, took 0.000995 sec.\n",
      "Epoch 393: T.cost 0.127186, Train acc 0.936264, test acc 0.973684, took 0.001 sec.\n",
      "Epoch 394: T.cost 0.126937, Train acc 0.936264, test acc 0.973684, took 0.000997 sec.\n",
      "Epoch 395: T.cost 0.126684, Train acc 0.938462, test acc 0.973684, took 0.000996 sec.\n",
      "Epoch 396: T.cost 0.126413, Train acc 0.940659, test acc 0.973684, took 0.000996 sec.\n",
      "Epoch 397: T.cost 0.126177, Train acc 0.940659, test acc 0.973684, took 0.00101 sec.\n",
      "Epoch 398: T.cost 0.125902, Train acc 0.940659, test acc 0.973684, took 0.000997 sec.\n",
      "New LR: 0.00408953512553\n",
      "Epoch 399: T.cost 0.125667, Train acc 0.940659, test acc 0.973684, took 0.00109 sec.\n",
      "Epoch 400: T.cost 0.125351, Train acc 0.940659, test acc 0.973684, took 0.001 sec.\n",
      "Epoch 401: T.cost 0.125070, Train acc 0.940659, test acc 0.973684, took 0.001 sec.\n",
      "Epoch 402: T.cost 0.124810, Train acc 0.940659, test acc 0.973684, took 0.000993 sec.\n",
      "Epoch 403: T.cost 0.124608, Train acc 0.940659, test acc 0.973684, took 0.000996 sec.\n",
      "Epoch 404: T.cost 0.124297, Train acc 0.942857, test acc 0.973684, took 0.000998 sec.\n",
      "Epoch 405: T.cost 0.124122, Train acc 0.942857, test acc 0.973684, took 0.00101 sec.\n",
      "Epoch 406: T.cost 0.123814, Train acc 0.942857, test acc 0.973684, took 0.000998 sec.\n",
      "Epoch 407: T.cost 0.123621, Train acc 0.942857, test acc 0.973684, took 0.001 sec.\n",
      "Epoch 408: T.cost 0.123354, Train acc 0.942857, test acc 0.973684, took 0.00101 sec.\n",
      "Epoch 409: T.cost 0.123120, Train acc 0.942857, test acc 0.973684, took 0.00101 sec.\n",
      "Epoch 410: T.cost 0.122891, Train acc 0.942857, test acc 0.973684, took 0.000998 sec.\n",
      "Epoch 411: T.cost 0.122630, Train acc 0.942857, test acc 0.973684, took 0.001 sec.\n",
      "Epoch 412: T.cost 0.122422, Train acc 0.942857, test acc 0.973684, took 0.00102 sec.\n",
      "Epoch 413: T.cost 0.122155, Train acc 0.942857, test acc 0.973684, took 0.001 sec.\n",
      "Epoch 414: T.cost 0.121949, Train acc 0.945055, test acc 0.973684, took 0.001 sec.\n",
      "Epoch 415: T.cost 0.121692, Train acc 0.945055, test acc 0.973684, took 0.001 sec.\n",
      "Epoch 416: T.cost 0.121477, Train acc 0.945055, test acc 0.973684, took 0.00102 sec.\n",
      "Epoch 417: T.cost 0.121236, Train acc 0.945055, test acc 0.973684, took 0.000998 sec.\n",
      "Epoch 418: T.cost 0.121008, Train acc 0.945055, test acc 0.973684, took 0.001 sec.\n",
      "New LR: 0.00404863986187\n",
      "Epoch 419: T.cost 0.120783, Train acc 0.945055, test acc 0.973684, took 0.00108 sec.\n",
      "Epoch 420: T.cost 0.120494, Train acc 0.945055, test acc 0.973684, took 0.00101 sec.\n",
      "Epoch 421: T.cost 0.120231, Train acc 0.945055, test acc 0.973684, took 0.000997 sec.\n",
      "Epoch 422: T.cost 0.119999, Train acc 0.947253, test acc 0.973684, took 0.000994 sec.\n",
      "Epoch 423: T.cost 0.119824, Train acc 0.947253, test acc 0.973684, took 0.000999 sec.\n",
      "Epoch 424: T.cost 0.119531, Train acc 0.947253, test acc 0.973684, took 0.00101 sec.\n",
      "Epoch 425: T.cost 0.119398, Train acc 0.947253, test acc 0.973684, took 0.000999 sec.\n",
      "Epoch 426: T.cost 0.119089, Train acc 0.947253, test acc 0.973684, took 0.000999 sec.\n",
      "Epoch 427: T.cost 0.118958, Train acc 0.947253, test acc 0.973684, took 0.001 sec.\n",
      "Epoch 428: T.cost 0.118668, Train acc 0.947253, test acc 0.973684, took 0.00101 sec.\n",
      "Epoch 429: T.cost 0.118514, Train acc 0.947253, test acc 0.973684, took 0.001 sec.\n",
      "Epoch 430: T.cost 0.118257, Train acc 0.947253, test acc 0.973684, took 0.000999 sec.\n",
      "Epoch 431: T.cost 0.118073, Train acc 0.947253, test acc 0.973684, took 0.001 sec.\n",
      "Epoch 432: T.cost 0.117851, Train acc 0.947253, test acc 0.973684, took 0.00101 sec.\n",
      "Epoch 433: T.cost 0.117640, Train acc 0.947253, test acc 0.973684, took 0.001 sec.\n",
      "Epoch 434: T.cost 0.117444, Train acc 0.947253, test acc 0.973684, took 0.001 sec.\n",
      "Epoch 435: T.cost 0.117219, Train acc 0.947253, test acc 0.973684, took 0.00101 sec.\n",
      "Epoch 436: T.cost 0.117038, Train acc 0.947253, test acc 0.973684, took 0.001 sec.\n",
      "Epoch 437: T.cost 0.116807, Train acc 0.947253, test acc 0.973684, took 0.000997 sec.\n",
      "Epoch 438: T.cost 0.116632, Train acc 0.947253, test acc 0.973684, took 0.000993 sec.\n",
      "New LR: 0.00400815350935\n",
      "Epoch 439: T.cost 0.116405, Train acc 0.947253, test acc 0.973684, took 0.00109 sec.\n",
      "Epoch 440: T.cost 0.116173, Train acc 0.947253, test acc 0.973684, took 0.00102 sec.\n",
      "Epoch 441: T.cost 0.115895, Train acc 0.947253, test acc 0.973684, took 0.000996 sec.\n",
      "Epoch 442: T.cost 0.115736, Train acc 0.947253, test acc 0.973684, took 0.000996 sec.\n",
      "Epoch 443: T.cost 0.115545, Train acc 0.947253, test acc 0.973684, took 0.00102 sec.\n",
      "Epoch 444: T.cost 0.115319, Train acc 0.947253, test acc 0.973684, took 0.000992 sec.\n",
      "Epoch 445: T.cost 0.115184, Train acc 0.947253, test acc 0.973684, took 0.000992 sec.\n",
      "Epoch 446: T.cost 0.114922, Train acc 0.947253, test acc 0.973684, took 0.000991 sec.\n",
      "Epoch 447: T.cost 0.114814, Train acc 0.947253, test acc 0.973684, took 0.00101 sec.\n",
      "Epoch 448: T.cost 0.114542, Train acc 0.947253, test acc 0.973684, took 0.001 sec.\n",
      "Epoch 449: T.cost 0.114441, Train acc 0.947253, test acc 0.973684, took 0.00099 sec.\n",
      "Epoch 450: T.cost 0.114175, Train acc 0.947253, test acc 0.973684, took 0.000991 sec.\n",
      "Epoch 451: T.cost 0.114067, Train acc 0.947253, test acc 0.973684, took 0.000996 sec.\n",
      "Epoch 452: T.cost 0.113817, Train acc 0.947253, test acc 0.973684, took 0.000989 sec.\n",
      "Epoch 453: T.cost 0.113697, Train acc 0.947253, test acc 0.964912, took 0.00099 sec.\n",
      "Epoch 454: T.cost 0.113466, Train acc 0.947253, test acc 0.964912, took 0.000989 sec.\n",
      "Epoch 455: T.cost 0.113332, Train acc 0.947253, test acc 0.964912, took 0.000991 sec.\n",
      "Epoch 456: T.cost 0.113119, Train acc 0.947253, test acc 0.964912, took 0.000986 sec.\n",
      "Epoch 457: T.cost 0.112973, Train acc 0.947253, test acc 0.964912, took 0.000991 sec.\n",
      "Epoch 458: T.cost 0.112777, Train acc 0.947253, test acc 0.964912, took 0.000995 sec.\n",
      "New LR: 0.00396807191893\n",
      "Epoch 459: T.cost 0.112621, Train acc 0.947253, test acc 0.964912, took 0.00109 sec.\n",
      "Epoch 460: T.cost 0.112381, Train acc 0.949451, test acc 0.964912, took 0.000993 sec.\n",
      "Epoch 461: T.cost 0.112146, Train acc 0.951648, test acc 0.964912, took 0.000989 sec.\n",
      "Epoch 462: T.cost 0.112012, Train acc 0.949451, test acc 0.964912, took 0.000996 sec.\n",
      "Epoch 463: T.cost 0.111841, Train acc 0.951648, test acc 0.964912, took 0.000995 sec.\n",
      "Epoch 464: T.cost 0.111659, Train acc 0.951648, test acc 0.964912, took 0.000991 sec.\n",
      "Epoch 465: T.cost 0.111531, Train acc 0.951648, test acc 0.964912, took 0.000994 sec.\n",
      "Epoch 466: T.cost 0.111321, Train acc 0.949451, test acc 0.964912, took 0.000994 sec.\n",
      "Epoch 467: T.cost 0.111219, Train acc 0.949451, test acc 0.964912, took 0.00107 sec.\n",
      "Epoch 468: T.cost 0.110995, Train acc 0.949451, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 469: T.cost 0.110906, Train acc 0.951648, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 470: T.cost 0.110678, Train acc 0.951648, test acc 0.964912, took 0.00106 sec.\n",
      "Epoch 471: T.cost 0.110594, Train acc 0.949451, test acc 0.964912, took 0.00106 sec.\n",
      "Epoch 472: T.cost 0.110370, Train acc 0.949451, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 473: T.cost 0.110286, Train acc 0.949451, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 474: T.cost 0.110067, Train acc 0.949451, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 475: T.cost 0.109982, Train acc 0.949451, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 476: T.cost 0.109770, Train acc 0.949451, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 477: T.cost 0.109682, Train acc 0.949451, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 478: T.cost 0.109478, Train acc 0.949451, test acc 0.964912, took 0.00105 sec.\n",
      "New LR: 0.00392839140259\n",
      "Epoch 479: T.cost 0.109388, Train acc 0.951648, test acc 0.964912, took 0.00112 sec.\n",
      "Epoch 480: T.cost 0.109133, Train acc 0.951648, test acc 0.956140, took 0.00105 sec.\n",
      "Epoch 481: T.cost 0.108956, Train acc 0.951648, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 482: T.cost 0.108819, Train acc 0.951648, test acc 0.956140, took 0.00104 sec.\n",
      "Epoch 483: T.cost 0.108702, Train acc 0.951648, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 484: T.cost 0.108521, Train acc 0.951648, test acc 0.956140, took 0.00104 sec.\n",
      "Epoch 485: T.cost 0.108443, Train acc 0.951648, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 486: T.cost 0.108237, Train acc 0.951648, test acc 0.956140, took 0.00105 sec.\n",
      "Epoch 487: T.cost 0.108182, Train acc 0.951648, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 488: T.cost 0.107963, Train acc 0.951648, test acc 0.956140, took 0.00105 sec.\n",
      "Epoch 489: T.cost 0.107920, Train acc 0.951648, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 490: T.cost 0.107697, Train acc 0.951648, test acc 0.956140, took 0.00104 sec.\n",
      "Epoch 491: T.cost 0.107661, Train acc 0.951648, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 492: T.cost 0.107437, Train acc 0.951648, test acc 0.956140, took 0.00104 sec.\n",
      "Epoch 493: T.cost 0.107405, Train acc 0.951648, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 494: T.cost 0.107183, Train acc 0.951648, test acc 0.956140, took 0.00104 sec.\n",
      "Epoch 495: T.cost 0.107152, Train acc 0.951648, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 496: T.cost 0.106934, Train acc 0.951648, test acc 0.956140, took 0.00106 sec.\n",
      "Epoch 497: T.cost 0.106903, Train acc 0.951648, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 498: T.cost 0.106690, Train acc 0.951648, test acc 0.956140, took 0.00103 sec.\n",
      "New LR: 0.00388910735026\n",
      "Epoch 499: T.cost 0.106657, Train acc 0.951648, test acc 0.964912, took 0.00112 sec.\n",
      "Epoch 500: T.cost 0.106391, Train acc 0.951648, test acc 0.956140, took 0.00105 sec.\n",
      "Epoch 501: T.cost 0.106265, Train acc 0.951648, test acc 0.956140, took 0.00105 sec.\n",
      "Epoch 502: T.cost 0.106124, Train acc 0.951648, test acc 0.956140, took 0.00104 sec.\n",
      "Epoch 503: T.cost 0.106058, Train acc 0.951648, test acc 0.956140, took 0.00104 sec.\n",
      "Epoch 504: T.cost 0.105873, Train acc 0.951648, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 505: T.cost 0.105844, Train acc 0.951648, test acc 0.956140, took 0.00104 sec.\n",
      "Epoch 506: T.cost 0.105636, Train acc 0.951648, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 507: T.cost 0.105627, Train acc 0.951648, test acc 0.956140, took 0.00106 sec.\n",
      "Epoch 508: T.cost 0.105409, Train acc 0.951648, test acc 0.956140, took 0.00105 sec.\n",
      "Epoch 509: T.cost 0.105410, Train acc 0.951648, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 510: T.cost 0.105189, Train acc 0.951648, test acc 0.956140, took 0.00105 sec.\n",
      "Epoch 511: T.cost 0.105193, Train acc 0.951648, test acc 0.956140, took 0.00105 sec.\n",
      "Epoch 512: T.cost 0.104976, Train acc 0.951648, test acc 0.956140, took 0.00104 sec.\n",
      "Epoch 513: T.cost 0.104979, Train acc 0.951648, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 514: T.cost 0.104768, Train acc 0.949451, test acc 0.956140, took 0.00104 sec.\n",
      "Epoch 515: T.cost 0.104766, Train acc 0.951648, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 516: T.cost 0.104566, Train acc 0.949451, test acc 0.956140, took 0.00104 sec.\n",
      "Epoch 517: T.cost 0.104555, Train acc 0.949451, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 518: T.cost 0.104370, Train acc 0.949451, test acc 0.956140, took 0.00103 sec.\n",
      "New LR: 0.00385021630442\n",
      "Epoch 519: T.cost 0.104347, Train acc 0.949451, test acc 0.964912, took 0.00112 sec.\n",
      "Epoch 520: T.cost 0.104120, Train acc 0.949451, test acc 0.956140, took 0.00104 sec.\n",
      "Epoch 521: T.cost 0.103989, Train acc 0.951648, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 522: T.cost 0.103892, Train acc 0.949451, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 523: T.cost 0.103824, Train acc 0.951648, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 524: T.cost 0.103685, Train acc 0.949451, test acc 0.956140, took 0.00105 sec.\n",
      "Epoch 525: T.cost 0.103648, Train acc 0.951648, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 526: T.cost 0.103495, Train acc 0.949451, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 527: T.cost 0.103466, Train acc 0.951648, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 528: T.cost 0.103317, Train acc 0.949451, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 529: T.cost 0.103281, Train acc 0.951648, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 530: T.cost 0.103150, Train acc 0.949451, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 531: T.cost 0.103094, Train acc 0.951648, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 532: T.cost 0.102992, Train acc 0.949451, test acc 0.956140, took 0.00104 sec.\n",
      "Epoch 533: T.cost 0.102907, Train acc 0.951648, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 534: T.cost 0.102843, Train acc 0.949451, test acc 0.956140, took 0.00104 sec.\n",
      "Epoch 535: T.cost 0.102719, Train acc 0.951648, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 536: T.cost 0.102703, Train acc 0.949451, test acc 0.956140, took 0.00106 sec.\n",
      "Epoch 537: T.cost 0.102532, Train acc 0.951648, test acc 0.973684, took 0.00104 sec.\n",
      "Epoch 538: T.cost 0.102572, Train acc 0.949451, test acc 0.956140, took 0.00103 sec.\n",
      "New LR: 0.00381171411602\n",
      "Epoch 539: T.cost 0.102344, Train acc 0.951648, test acc 0.973684, took 0.00111 sec.\n",
      "Epoch 540: T.cost 0.102393, Train acc 0.949451, test acc 0.956140, took 0.00104 sec.\n",
      "Epoch 541: T.cost 0.102024, Train acc 0.951648, test acc 0.973684, took 0.00103 sec.\n",
      "Epoch 542: T.cost 0.102208, Train acc 0.949451, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 543: T.cost 0.101902, Train acc 0.951648, test acc 0.973684, took 0.00103 sec.\n",
      "Epoch 544: T.cost 0.102059, Train acc 0.949451, test acc 0.956140, took 0.00104 sec.\n",
      "Epoch 545: T.cost 0.101760, Train acc 0.951648, test acc 0.973684, took 0.00103 sec.\n",
      "Epoch 546: T.cost 0.101939, Train acc 0.949451, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 547: T.cost 0.101607, Train acc 0.951648, test acc 0.973684, took 0.00103 sec.\n",
      "Epoch 548: T.cost 0.101840, Train acc 0.949451, test acc 0.956140, took 0.00105 sec.\n",
      "Epoch 549: T.cost 0.101447, Train acc 0.951648, test acc 0.973684, took 0.00104 sec.\n",
      "Epoch 550: T.cost 0.101760, Train acc 0.949451, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 551: T.cost 0.101284, Train acc 0.951648, test acc 0.973684, took 0.00103 sec.\n",
      "Epoch 552: T.cost 0.101697, Train acc 0.949451, test acc 0.956140, took 0.00105 sec.\n",
      "Epoch 553: T.cost 0.101118, Train acc 0.951648, test acc 0.973684, took 0.00103 sec.\n",
      "Epoch 554: T.cost 0.101650, Train acc 0.949451, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 555: T.cost 0.100951, Train acc 0.951648, test acc 0.973684, took 0.00106 sec.\n",
      "Epoch 556: T.cost 0.101620, Train acc 0.949451, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 557: T.cost 0.100784, Train acc 0.951648, test acc 0.973684, took 0.0023 sec.\n",
      "Epoch 558: T.cost 0.101605, Train acc 0.949451, test acc 0.956140, took 0.00151 sec.\n",
      "New LR: 0.00377359686652\n",
      "Epoch 559: T.cost 0.100618, Train acc 0.951648, test acc 0.973684, took 0.00123 sec.\n",
      "Epoch 560: T.cost 0.101553, Train acc 0.949451, test acc 0.956140, took 0.00104 sec.\n",
      "Epoch 561: T.cost 0.100368, Train acc 0.953846, test acc 0.973684, took 0.00104 sec.\n",
      "Epoch 562: T.cost 0.101433, Train acc 0.949451, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 563: T.cost 0.100321, Train acc 0.953846, test acc 0.973684, took 0.00103 sec.\n",
      "Epoch 564: T.cost 0.101382, Train acc 0.947253, test acc 0.956140, took 0.00104 sec.\n",
      "Epoch 565: T.cost 0.100244, Train acc 0.956044, test acc 0.973684, took 0.00104 sec.\n",
      "Epoch 566: T.cost 0.101383, Train acc 0.949451, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 567: T.cost 0.100152, Train acc 0.956044, test acc 0.973684, took 0.00103 sec.\n",
      "Epoch 568: T.cost 0.101421, Train acc 0.949451, test acc 0.956140, took 0.00105 sec.\n",
      "Epoch 569: T.cost 0.100053, Train acc 0.956044, test acc 0.973684, took 0.00102 sec.\n",
      "Epoch 570: T.cost 0.101492, Train acc 0.949451, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 571: T.cost 0.099953, Train acc 0.956044, test acc 0.973684, took 0.00103 sec.\n",
      "Epoch 572: T.cost 0.101590, Train acc 0.949451, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 573: T.cost 0.099856, Train acc 0.956044, test acc 0.982456, took 0.00102 sec.\n",
      "Epoch 574: T.cost 0.101712, Train acc 0.949451, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 575: T.cost 0.099764, Train acc 0.956044, test acc 0.982456, took 0.00103 sec.\n",
      "Epoch 576: T.cost 0.101853, Train acc 0.949451, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 577: T.cost 0.099680, Train acc 0.956044, test acc 0.982456, took 0.00104 sec.\n",
      "Epoch 578: T.cost 0.102014, Train acc 0.949451, test acc 0.956140, took 0.00103 sec.\n",
      "New LR: 0.00373586086789\n",
      "Epoch 579: T.cost 0.099607, Train acc 0.956044, test acc 0.982456, took 0.00112 sec.\n",
      "Epoch 580: T.cost 0.102146, Train acc 0.947253, test acc 0.956140, took 0.00105 sec.\n",
      "Epoch 581: T.cost 0.099563, Train acc 0.953846, test acc 0.982456, took 0.00103 sec.\n",
      "Epoch 582: T.cost 0.102101, Train acc 0.945055, test acc 0.956140, took 0.00104 sec.\n",
      "Epoch 583: T.cost 0.099737, Train acc 0.953846, test acc 0.982456, took 0.00105 sec.\n",
      "Epoch 584: T.cost 0.102187, Train acc 0.945055, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 585: T.cost 0.099885, Train acc 0.953846, test acc 0.982456, took 0.00101 sec.\n",
      "Epoch 586: T.cost 0.102365, Train acc 0.947253, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 587: T.cost 0.100032, Train acc 0.953846, test acc 0.982456, took 0.00103 sec.\n",
      "Epoch 588: T.cost 0.102613, Train acc 0.947253, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 589: T.cost 0.100196, Train acc 0.953846, test acc 0.982456, took 0.00102 sec.\n",
      "Epoch 590: T.cost 0.102916, Train acc 0.947253, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 591: T.cost 0.100392, Train acc 0.953846, test acc 0.982456, took 0.00105 sec.\n",
      "Epoch 592: T.cost 0.103265, Train acc 0.949451, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 593: T.cost 0.100633, Train acc 0.951648, test acc 0.982456, took 0.00102 sec.\n",
      "Epoch 594: T.cost 0.103653, Train acc 0.951648, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 595: T.cost 0.100932, Train acc 0.951648, test acc 0.982456, took 0.00103 sec.\n",
      "Epoch 596: T.cost 0.104078, Train acc 0.953846, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 597: T.cost 0.101302, Train acc 0.953846, test acc 0.982456, took 0.00103 sec.\n",
      "Epoch 598: T.cost 0.104539, Train acc 0.953846, test acc 0.956140, took 0.00104 sec.\n",
      "New LR: 0.00369850220159\n",
      "Epoch 599: T.cost 0.101759, Train acc 0.953846, test acc 0.982456, took 0.0011 sec.\n",
      "Epoch 600: T.cost 0.105024, Train acc 0.953846, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 601: T.cost 0.102477, Train acc 0.953846, test acc 0.982456, took 0.00102 sec.\n",
      "Epoch 602: T.cost 0.105153, Train acc 0.953846, test acc 0.956140, took 0.00105 sec.\n",
      "Epoch 603: T.cost 0.103558, Train acc 0.953846, test acc 0.982456, took 0.00103 sec.\n",
      "Epoch 604: T.cost 0.105517, Train acc 0.951648, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 605: T.cost 0.104723, Train acc 0.953846, test acc 0.982456, took 0.00103 sec.\n",
      "Epoch 606: T.cost 0.106040, Train acc 0.951648, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 607: T.cost 0.106024, Train acc 0.958242, test acc 0.982456, took 0.00103 sec.\n",
      "Epoch 608: T.cost 0.106668, Train acc 0.951648, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 609: T.cost 0.107504, Train acc 0.958242, test acc 0.982456, took 0.00102 sec.\n",
      "Epoch 610: T.cost 0.107359, Train acc 0.951648, test acc 0.956140, took 0.00105 sec.\n",
      "Epoch 611: T.cost 0.109192, Train acc 0.958242, test acc 0.973684, took 0.00102 sec.\n",
      "Epoch 612: T.cost 0.108072, Train acc 0.951648, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 613: T.cost 0.111104, Train acc 0.958242, test acc 0.973684, took 0.00103 sec.\n",
      "Epoch 614: T.cost 0.108756, Train acc 0.956044, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 615: T.cost 0.113228, Train acc 0.958242, test acc 0.973684, took 0.00103 sec.\n",
      "Epoch 616: T.cost 0.109352, Train acc 0.956044, test acc 0.956140, took 0.00104 sec.\n",
      "Epoch 617: T.cost 0.115508, Train acc 0.958242, test acc 0.982456, took 0.00103 sec.\n",
      "Epoch 618: T.cost 0.109772, Train acc 0.958242, test acc 0.956140, took 0.00103 sec.\n",
      "New LR: 0.00366151717957\n",
      "Epoch 619: T.cost 0.117828, Train acc 0.958242, test acc 0.982456, took 0.00111 sec.\n",
      "Epoch 620: T.cost 0.109916, Train acc 0.958242, test acc 0.956140, took 0.00104 sec.\n",
      "Epoch 621: T.cost 0.119910, Train acc 0.956044, test acc 0.982456, took 0.00104 sec.\n",
      "Epoch 622: T.cost 0.108848, Train acc 0.958242, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 623: T.cost 0.121543, Train acc 0.956044, test acc 0.982456, took 0.00103 sec.\n",
      "Epoch 624: T.cost 0.107113, Train acc 0.956044, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 625: T.cost 0.121708, Train acc 0.956044, test acc 0.982456, took 0.00102 sec.\n",
      "Epoch 626: T.cost 0.104519, Train acc 0.956044, test acc 0.956140, took 0.00104 sec.\n",
      "Epoch 627: T.cost 0.120231, Train acc 0.956044, test acc 0.982456, took 0.00104 sec.\n",
      "Epoch 628: T.cost 0.101170, Train acc 0.960440, test acc 0.956140, took 0.00105 sec.\n",
      "Epoch 629: T.cost 0.117240, Train acc 0.956044, test acc 0.964912, took 0.00102 sec.\n",
      "Epoch 630: T.cost 0.097561, Train acc 0.960440, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 631: T.cost 0.113244, Train acc 0.953846, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 632: T.cost 0.094433, Train acc 0.960440, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 633: T.cost 0.108952, Train acc 0.949451, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 634: T.cost 0.092406, Train acc 0.958242, test acc 0.964912, took 0.00102 sec.\n",
      "Epoch 635: T.cost 0.104988, Train acc 0.947253, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 636: T.cost 0.091665, Train acc 0.960440, test acc 0.973684, took 0.00103 sec.\n",
      "Epoch 637: T.cost 0.101709, Train acc 0.949451, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 638: T.cost 0.091937, Train acc 0.958242, test acc 0.973684, took 0.00102 sec.\n",
      "New LR: 0.00362490211381\n",
      "Epoch 639: T.cost 0.099203, Train acc 0.949451, test acc 0.956140, took 0.00114 sec.\n",
      "Epoch 640: T.cost 0.092733, Train acc 0.953846, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 641: T.cost 0.097222, Train acc 0.949451, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 642: T.cost 0.093663, Train acc 0.953846, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 643: T.cost 0.095961, Train acc 0.949451, test acc 0.956140, took 0.00104 sec.\n",
      "Epoch 644: T.cost 0.094478, Train acc 0.953846, test acc 0.956140, took 0.00106 sec.\n",
      "Epoch 645: T.cost 0.095087, Train acc 0.949451, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 646: T.cost 0.095050, Train acc 0.953846, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 647: T.cost 0.094502, Train acc 0.951648, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 648: T.cost 0.095369, Train acc 0.953846, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 649: T.cost 0.094126, Train acc 0.951648, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 650: T.cost 0.095484, Train acc 0.953846, test acc 0.956140, took 0.00104 sec.\n",
      "Epoch 651: T.cost 0.093896, Train acc 0.951648, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 652: T.cost 0.095458, Train acc 0.953846, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 653: T.cost 0.093763, Train acc 0.951648, test acc 0.956140, took 0.00104 sec.\n",
      "Epoch 654: T.cost 0.095348, Train acc 0.953846, test acc 0.956140, took 0.00105 sec.\n",
      "Epoch 655: T.cost 0.093692, Train acc 0.951648, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 656: T.cost 0.095194, Train acc 0.953846, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 657: T.cost 0.093655, Train acc 0.951648, test acc 0.956140, took 0.00104 sec.\n",
      "Epoch 658: T.cost 0.095024, Train acc 0.953846, test acc 0.956140, took 0.00103 sec.\n",
      "New LR: 0.00358865308575\n",
      "Epoch 659: T.cost 0.093636, Train acc 0.951648, test acc 0.956140, took 0.00111 sec.\n",
      "Epoch 660: T.cost 0.094791, Train acc 0.953846, test acc 0.956140, took 0.00104 sec.\n",
      "Epoch 661: T.cost 0.093507, Train acc 0.953846, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 662: T.cost 0.094619, Train acc 0.953846, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 663: T.cost 0.093504, Train acc 0.953846, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 664: T.cost 0.094451, Train acc 0.953846, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 665: T.cost 0.093503, Train acc 0.953846, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 666: T.cost 0.094295, Train acc 0.953846, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 667: T.cost 0.093499, Train acc 0.953846, test acc 0.956140, took 0.00104 sec.\n",
      "Epoch 668: T.cost 0.094153, Train acc 0.953846, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 669: T.cost 0.093488, Train acc 0.953846, test acc 0.956140, took 0.00104 sec.\n",
      "Epoch 670: T.cost 0.094024, Train acc 0.953846, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 671: T.cost 0.093470, Train acc 0.953846, test acc 0.956140, took 0.00104 sec.\n",
      "Epoch 672: T.cost 0.093907, Train acc 0.953846, test acc 0.956140, took 0.00104 sec.\n",
      "Epoch 673: T.cost 0.093444, Train acc 0.953846, test acc 0.956140, took 0.00104 sec.\n",
      "Epoch 674: T.cost 0.093801, Train acc 0.953846, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 675: T.cost 0.093411, Train acc 0.953846, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 676: T.cost 0.093705, Train acc 0.953846, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 677: T.cost 0.093373, Train acc 0.953846, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 678: T.cost 0.093615, Train acc 0.953846, test acc 0.956140, took 0.00102 sec.\n",
      "New LR: 0.00355276663788\n",
      "Epoch 679: T.cost 0.093330, Train acc 0.953846, test acc 0.956140, took 0.00111 sec.\n",
      "Epoch 680: T.cost 0.093482, Train acc 0.953846, test acc 0.956140, took 0.00105 sec.\n",
      "Epoch 681: T.cost 0.093166, Train acc 0.953846, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 682: T.cost 0.093383, Train acc 0.953846, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 683: T.cost 0.093135, Train acc 0.953846, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 684: T.cost 0.093294, Train acc 0.953846, test acc 0.956140, took 0.00105 sec.\n",
      "Epoch 685: T.cost 0.093096, Train acc 0.953846, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 686: T.cost 0.093213, Train acc 0.953846, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 687: T.cost 0.093053, Train acc 0.953846, test acc 0.956140, took 0.00104 sec.\n",
      "Epoch 688: T.cost 0.093138, Train acc 0.953846, test acc 0.956140, took 0.00104 sec.\n",
      "Epoch 689: T.cost 0.093005, Train acc 0.953846, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 690: T.cost 0.093068, Train acc 0.953846, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 691: T.cost 0.092955, Train acc 0.953846, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 692: T.cost 0.093001, Train acc 0.953846, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 693: T.cost 0.092902, Train acc 0.953846, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 694: T.cost 0.092936, Train acc 0.953846, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 695: T.cost 0.092848, Train acc 0.953846, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 696: T.cost 0.092874, Train acc 0.953846, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 697: T.cost 0.092793, Train acc 0.953846, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 698: T.cost 0.092812, Train acc 0.953846, test acc 0.956140, took 0.00102 sec.\n",
      "New LR: 0.00351723908214\n",
      "Epoch 699: T.cost 0.092738, Train acc 0.953846, test acc 0.956140, took 0.00112 sec.\n",
      "Epoch 700: T.cost 0.092703, Train acc 0.953846, test acc 0.956140, took 0.00104 sec.\n",
      "Epoch 701: T.cost 0.092560, Train acc 0.953846, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 702: T.cost 0.092620, Train acc 0.953846, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 703: T.cost 0.092524, Train acc 0.953846, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 704: T.cost 0.092547, Train acc 0.953846, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 705: T.cost 0.092482, Train acc 0.953846, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 706: T.cost 0.092480, Train acc 0.953846, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 707: T.cost 0.092435, Train acc 0.953846, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 708: T.cost 0.092417, Train acc 0.953846, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 709: T.cost 0.092386, Train acc 0.953846, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 710: T.cost 0.092358, Train acc 0.953846, test acc 0.956140, took 0.00104 sec.\n",
      "Epoch 711: T.cost 0.092335, Train acc 0.953846, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 712: T.cost 0.092301, Train acc 0.953846, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 713: T.cost 0.092282, Train acc 0.953846, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 714: T.cost 0.092244, Train acc 0.953846, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 715: T.cost 0.092230, Train acc 0.953846, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 716: T.cost 0.092189, Train acc 0.953846, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 717: T.cost 0.092177, Train acc 0.953846, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 718: T.cost 0.092135, Train acc 0.953846, test acc 0.956140, took 0.00104 sec.\n",
      "New LR: 0.0034820667305\n",
      "Epoch 719: T.cost 0.092123, Train acc 0.953846, test acc 0.956140, took 0.00111 sec.\n",
      "Epoch 720: T.cost 0.092031, Train acc 0.953846, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 721: T.cost 0.091944, Train acc 0.953846, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 722: T.cost 0.091953, Train acc 0.953846, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 723: T.cost 0.091912, Train acc 0.953846, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 724: T.cost 0.091885, Train acc 0.953846, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 725: T.cost 0.091874, Train acc 0.953846, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 726: T.cost 0.091823, Train acc 0.953846, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 727: T.cost 0.091830, Train acc 0.953846, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 728: T.cost 0.091766, Train acc 0.953846, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 729: T.cost 0.091784, Train acc 0.953846, test acc 0.956140, took 0.00104 sec.\n",
      "Epoch 730: T.cost 0.091711, Train acc 0.953846, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 731: T.cost 0.091736, Train acc 0.953846, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 732: T.cost 0.091658, Train acc 0.953846, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 733: T.cost 0.091687, Train acc 0.953846, test acc 0.956140, took 0.00104 sec.\n",
      "Epoch 734: T.cost 0.091606, Train acc 0.953846, test acc 0.956140, took 0.00104 sec.\n",
      "Epoch 735: T.cost 0.091638, Train acc 0.953846, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 736: T.cost 0.091556, Train acc 0.953846, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 737: T.cost 0.091588, Train acc 0.953846, test acc 0.956140, took 0.00106 sec.\n",
      "Epoch 738: T.cost 0.091506, Train acc 0.953846, test acc 0.956140, took 0.00102 sec.\n",
      "New LR: 0.00344724612543\n",
      "Epoch 739: T.cost 0.091538, Train acc 0.953846, test acc 0.956140, took 0.00111 sec.\n",
      "Epoch 740: T.cost 0.091407, Train acc 0.953846, test acc 0.956140, took 0.00106 sec.\n",
      "Epoch 741: T.cost 0.091360, Train acc 0.953846, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 742: T.cost 0.091329, Train acc 0.953846, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 743: T.cost 0.091335, Train acc 0.953846, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 744: T.cost 0.091264, Train acc 0.953846, test acc 0.956140, took 0.00126 sec.\n",
      "Epoch 745: T.cost 0.091301, Train acc 0.953846, test acc 0.956140, took 0.00134 sec.\n",
      "Epoch 746: T.cost 0.091205, Train acc 0.953846, test acc 0.956140, took 0.00156 sec.\n",
      "Epoch 747: T.cost 0.091261, Train acc 0.953846, test acc 0.956140, took 0.00105 sec.\n",
      "Epoch 748: T.cost 0.091151, Train acc 0.953846, test acc 0.956140, took 0.00104 sec.\n",
      "Epoch 749: T.cost 0.091219, Train acc 0.953846, test acc 0.956140, took 0.00105 sec.\n",
      "Epoch 750: T.cost 0.091100, Train acc 0.953846, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 751: T.cost 0.091175, Train acc 0.953846, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 752: T.cost 0.091051, Train acc 0.953846, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 753: T.cost 0.091129, Train acc 0.953846, test acc 0.956140, took 0.00104 sec.\n",
      "Epoch 754: T.cost 0.091003, Train acc 0.953846, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 755: T.cost 0.091083, Train acc 0.953846, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 756: T.cost 0.090957, Train acc 0.953846, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 757: T.cost 0.091036, Train acc 0.953846, test acc 0.956140, took 0.00105 sec.\n",
      "Epoch 758: T.cost 0.090911, Train acc 0.953846, test acc 0.956140, took 0.00102 sec.\n",
      "New LR: 0.00341277357889\n",
      "Epoch 759: T.cost 0.090989, Train acc 0.953846, test acc 0.956140, took 0.00112 sec.\n",
      "Epoch 760: T.cost 0.090818, Train acc 0.953846, test acc 0.956140, took 0.00105 sec.\n",
      "Epoch 761: T.cost 0.090816, Train acc 0.953846, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 762: T.cost 0.090739, Train acc 0.953846, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 763: T.cost 0.090800, Train acc 0.953846, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 764: T.cost 0.090673, Train acc 0.953846, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 765: T.cost 0.090773, Train acc 0.953846, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 766: T.cost 0.090617, Train acc 0.953846, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 767: T.cost 0.090739, Train acc 0.953846, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 768: T.cost 0.090567, Train acc 0.953846, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 769: T.cost 0.090702, Train acc 0.953846, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 770: T.cost 0.090521, Train acc 0.953846, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 771: T.cost 0.090662, Train acc 0.953846, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 772: T.cost 0.090478, Train acc 0.953846, test acc 0.956140, took 0.00105 sec.\n",
      "Epoch 773: T.cost 0.090621, Train acc 0.953846, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 774: T.cost 0.090438, Train acc 0.953846, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 775: T.cost 0.090580, Train acc 0.953846, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 776: T.cost 0.090399, Train acc 0.953846, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 777: T.cost 0.090537, Train acc 0.953846, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 778: T.cost 0.090363, Train acc 0.953846, test acc 0.956140, took 0.00104 sec.\n",
      "New LR: 0.00337864586385\n",
      "Epoch 779: T.cost 0.090495, Train acc 0.953846, test acc 0.964912, took 0.00112 sec.\n",
      "Epoch 780: T.cost 0.090283, Train acc 0.953846, test acc 0.956140, took 0.00104 sec.\n",
      "Epoch 781: T.cost 0.090336, Train acc 0.953846, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 782: T.cost 0.090198, Train acc 0.953846, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 783: T.cost 0.090339, Train acc 0.953846, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 784: T.cost 0.090136, Train acc 0.953846, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 785: T.cost 0.090327, Train acc 0.953846, test acc 0.964912, took 0.00102 sec.\n",
      "Epoch 786: T.cost 0.090087, Train acc 0.953846, test acc 0.956140, took 0.00104 sec.\n",
      "Epoch 787: T.cost 0.090307, Train acc 0.953846, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 788: T.cost 0.090048, Train acc 0.953846, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 789: T.cost 0.090283, Train acc 0.953846, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 790: T.cost 0.090018, Train acc 0.953846, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 791: T.cost 0.090256, Train acc 0.953846, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 792: T.cost 0.089994, Train acc 0.953846, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 793: T.cost 0.090229, Train acc 0.953846, test acc 0.964912, took 0.00102 sec.\n",
      "Epoch 794: T.cost 0.089976, Train acc 0.953846, test acc 0.956140, took 0.00104 sec.\n",
      "Epoch 795: T.cost 0.090202, Train acc 0.953846, test acc 0.964912, took 0.00102 sec.\n",
      "Epoch 796: T.cost 0.089965, Train acc 0.953846, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 797: T.cost 0.090177, Train acc 0.953846, test acc 0.964912, took 0.00102 sec.\n",
      "Epoch 798: T.cost 0.089960, Train acc 0.953846, test acc 0.956140, took 0.00104 sec.\n",
      "New LR: 0.00334485929227\n",
      "Epoch 799: T.cost 0.090154, Train acc 0.953846, test acc 0.964912, took 0.00111 sec.\n",
      "Epoch 800: T.cost 0.089924, Train acc 0.953846, test acc 0.956140, took 0.00104 sec.\n",
      "Epoch 801: T.cost 0.090046, Train acc 0.953846, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 802: T.cost 0.089842, Train acc 0.953846, test acc 0.956140, took 0.00104 sec.\n",
      "Epoch 803: T.cost 0.090109, Train acc 0.953846, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 804: T.cost 0.089802, Train acc 0.953846, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 805: T.cost 0.090153, Train acc 0.953846, test acc 0.964912, took 0.00102 sec.\n",
      "Epoch 806: T.cost 0.089792, Train acc 0.953846, test acc 0.956140, took 0.00104 sec.\n",
      "Epoch 807: T.cost 0.090189, Train acc 0.953846, test acc 0.964912, took 0.00102 sec.\n",
      "Epoch 808: T.cost 0.089805, Train acc 0.953846, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 809: T.cost 0.090224, Train acc 0.953846, test acc 0.973684, took 0.00103 sec.\n",
      "Epoch 810: T.cost 0.089839, Train acc 0.951648, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 811: T.cost 0.090264, Train acc 0.953846, test acc 0.973684, took 0.00103 sec.\n",
      "Epoch 812: T.cost 0.089892, Train acc 0.953846, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 813: T.cost 0.090314, Train acc 0.953846, test acc 0.973684, took 0.00103 sec.\n",
      "Epoch 814: T.cost 0.089967, Train acc 0.953846, test acc 0.956140, took 0.00104 sec.\n",
      "Epoch 815: T.cost 0.090377, Train acc 0.953846, test acc 0.973684, took 0.00102 sec.\n",
      "Epoch 816: T.cost 0.090064, Train acc 0.956044, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 817: T.cost 0.090458, Train acc 0.953846, test acc 0.973684, took 0.00106 sec.\n",
      "Epoch 818: T.cost 0.090186, Train acc 0.956044, test acc 0.956140, took 0.00102 sec.\n",
      "New LR: 0.00331141063711\n",
      "Epoch 819: T.cost 0.090561, Train acc 0.951648, test acc 0.973684, took 0.00113 sec.\n",
      "Epoch 820: T.cost 0.090310, Train acc 0.958242, test acc 0.956140, took 0.00104 sec.\n",
      "Epoch 821: T.cost 0.090660, Train acc 0.951648, test acc 0.973684, took 0.00103 sec.\n",
      "Epoch 822: T.cost 0.090304, Train acc 0.958242, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 823: T.cost 0.090970, Train acc 0.951648, test acc 0.973684, took 0.00103 sec.\n",
      "Epoch 824: T.cost 0.090389, Train acc 0.958242, test acc 0.956140, took 0.00104 sec.\n",
      "Epoch 825: T.cost 0.091269, Train acc 0.951648, test acc 0.973684, took 0.00102 sec.\n",
      "Epoch 826: T.cost 0.090543, Train acc 0.958242, test acc 0.956140, took 0.00104 sec.\n",
      "Epoch 827: T.cost 0.091582, Train acc 0.951648, test acc 0.973684, took 0.00104 sec.\n",
      "Epoch 828: T.cost 0.090756, Train acc 0.958242, test acc 0.956140, took 0.00106 sec.\n",
      "Epoch 829: T.cost 0.091929, Train acc 0.951648, test acc 0.982456, took 0.00102 sec.\n",
      "Epoch 830: T.cost 0.091022, Train acc 0.958242, test acc 0.956140, took 0.00104 sec.\n",
      "Epoch 831: T.cost 0.092325, Train acc 0.951648, test acc 0.982456, took 0.00103 sec.\n",
      "Epoch 832: T.cost 0.091339, Train acc 0.960440, test acc 0.956140, took 0.00105 sec.\n",
      "Epoch 833: T.cost 0.092783, Train acc 0.951648, test acc 0.982456, took 0.00102 sec.\n",
      "Epoch 834: T.cost 0.091705, Train acc 0.960440, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 835: T.cost 0.093316, Train acc 0.953846, test acc 0.982456, took 0.00104 sec.\n",
      "Epoch 836: T.cost 0.092119, Train acc 0.960440, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 837: T.cost 0.093937, Train acc 0.956044, test acc 0.982456, took 0.00102 sec.\n",
      "Epoch 838: T.cost 0.092579, Train acc 0.962637, test acc 0.956140, took 0.00104 sec.\n",
      "New LR: 0.00327829644084\n",
      "Epoch 839: T.cost 0.094657, Train acc 0.956044, test acc 0.982456, took 0.00113 sec.\n",
      "Epoch 840: T.cost 0.093086, Train acc 0.960440, test acc 0.956140, took 0.00104 sec.\n",
      "Epoch 841: T.cost 0.095532, Train acc 0.958242, test acc 0.982456, took 0.00103 sec.\n",
      "Epoch 842: T.cost 0.093239, Train acc 0.962637, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 843: T.cost 0.096699, Train acc 0.958242, test acc 0.982456, took 0.00104 sec.\n",
      "Epoch 844: T.cost 0.093487, Train acc 0.962637, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 845: T.cost 0.097823, Train acc 0.956044, test acc 0.982456, took 0.00103 sec.\n",
      "Epoch 846: T.cost 0.093764, Train acc 0.964835, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 847: T.cost 0.098920, Train acc 0.956044, test acc 0.982456, took 0.00103 sec.\n",
      "Epoch 848: T.cost 0.094015, Train acc 0.964835, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 849: T.cost 0.099982, Train acc 0.956044, test acc 0.982456, took 0.00103 sec.\n",
      "Epoch 850: T.cost 0.094192, Train acc 0.964835, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 851: T.cost 0.101414, Train acc 0.958242, test acc 0.982456, took 0.00102 sec.\n",
      "Epoch 852: T.cost 0.094384, Train acc 0.964835, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 853: T.cost 0.102664, Train acc 0.958242, test acc 0.982456, took 0.00102 sec.\n",
      "Epoch 854: T.cost 0.094312, Train acc 0.964835, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 855: T.cost 0.103435, Train acc 0.958242, test acc 0.982456, took 0.00102 sec.\n",
      "Epoch 856: T.cost 0.093839, Train acc 0.964835, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 857: T.cost 0.103863, Train acc 0.956044, test acc 0.982456, took 0.00103 sec.\n",
      "Epoch 858: T.cost 0.093035, Train acc 0.964835, test acc 0.956140, took 0.00103 sec.\n",
      "New LR: 0.00324551347643\n",
      "Epoch 859: T.cost 0.103900, Train acc 0.956044, test acc 0.973684, took 0.00111 sec.\n",
      "Epoch 860: T.cost 0.091916, Train acc 0.964835, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 861: T.cost 0.103307, Train acc 0.956044, test acc 0.973684, took 0.00103 sec.\n",
      "Epoch 862: T.cost 0.090023, Train acc 0.964835, test acc 0.956140, took 0.00103 sec.\n",
      "Epoch 863: T.cost 0.102306, Train acc 0.958242, test acc 0.973684, took 0.00102 sec.\n",
      "Epoch 864: T.cost 0.088020, Train acc 0.962637, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 865: T.cost 0.100486, Train acc 0.958242, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 866: T.cost 0.086137, Train acc 0.967033, test acc 0.956140, took 0.00102 sec.\n",
      "Epoch 867: T.cost 0.098148, Train acc 0.956044, test acc 0.964912, took 0.00102 sec.\n",
      "Epoch 868: T.cost 0.084614, Train acc 0.967033, test acc 0.964912, took 0.00102 sec.\n",
      "Epoch 869: T.cost 0.095620, Train acc 0.958242, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 870: T.cost 0.083605, Train acc 0.962637, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 871: T.cost 0.093190, Train acc 0.958242, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 872: T.cost 0.083130, Train acc 0.964835, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 873: T.cost 0.091050, Train acc 0.956044, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 874: T.cost 0.083089, Train acc 0.964835, test acc 0.964912, took 0.00102 sec.\n",
      "Epoch 875: T.cost 0.089293, Train acc 0.956044, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 876: T.cost 0.083321, Train acc 0.962637, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 877: T.cost 0.087920, Train acc 0.956044, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 878: T.cost 0.083670, Train acc 0.960440, test acc 0.964912, took 0.00104 sec.\n",
      "New LR: 0.00321305828635\n",
      "Epoch 879: T.cost 0.086886, Train acc 0.956044, test acc 0.964912, took 0.00112 sec.\n",
      "Epoch 880: T.cost 0.083994, Train acc 0.960440, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 881: T.cost 0.086005, Train acc 0.958242, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 882: T.cost 0.084253, Train acc 0.958242, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 883: T.cost 0.085474, Train acc 0.958242, test acc 0.964912, took 0.00106 sec.\n",
      "Epoch 884: T.cost 0.084459, Train acc 0.958242, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 885: T.cost 0.085074, Train acc 0.958242, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 886: T.cost 0.084598, Train acc 0.958242, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 887: T.cost 0.084777, Train acc 0.958242, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 888: T.cost 0.084675, Train acc 0.958242, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 889: T.cost 0.084555, Train acc 0.960440, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 890: T.cost 0.084701, Train acc 0.958242, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 891: T.cost 0.084390, Train acc 0.960440, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 892: T.cost 0.084690, Train acc 0.958242, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 893: T.cost 0.084263, Train acc 0.960440, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 894: T.cost 0.084655, Train acc 0.958242, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 895: T.cost 0.084163, Train acc 0.960440, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 896: T.cost 0.084606, Train acc 0.958242, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 897: T.cost 0.084081, Train acc 0.960440, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 898: T.cost 0.084549, Train acc 0.958242, test acc 0.964912, took 0.00104 sec.\n",
      "New LR: 0.00318092764355\n",
      "Epoch 899: T.cost 0.084011, Train acc 0.960440, test acc 0.964912, took 0.00112 sec.\n",
      "Epoch 900: T.cost 0.084437, Train acc 0.958242, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 901: T.cost 0.083843, Train acc 0.960440, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 902: T.cost 0.084350, Train acc 0.958242, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 903: T.cost 0.083810, Train acc 0.960440, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 904: T.cost 0.084270, Train acc 0.958242, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 905: T.cost 0.083777, Train acc 0.960440, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 906: T.cost 0.084196, Train acc 0.958242, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 907: T.cost 0.083744, Train acc 0.960440, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 908: T.cost 0.084127, Train acc 0.960440, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 909: T.cost 0.083711, Train acc 0.960440, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 910: T.cost 0.084066, Train acc 0.960440, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 911: T.cost 0.083676, Train acc 0.962637, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 912: T.cost 0.084010, Train acc 0.960440, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 913: T.cost 0.083641, Train acc 0.962637, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 914: T.cost 0.083959, Train acc 0.960440, test acc 0.964912, took 0.00106 sec.\n",
      "Epoch 915: T.cost 0.083604, Train acc 0.962637, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 916: T.cost 0.083913, Train acc 0.962637, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 917: T.cost 0.083566, Train acc 0.962637, test acc 0.964912, took 0.00106 sec.\n",
      "Epoch 918: T.cost 0.083871, Train acc 0.962637, test acc 0.964912, took 0.00104 sec.\n",
      "New LR: 0.00314911832102\n",
      "Epoch 919: T.cost 0.083528, Train acc 0.962637, test acc 0.964912, took 0.00112 sec.\n",
      "Epoch 920: T.cost 0.083782, Train acc 0.962637, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 921: T.cost 0.083383, Train acc 0.962637, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 922: T.cost 0.083716, Train acc 0.962637, test acc 0.964912, took 0.00106 sec.\n",
      "Epoch 923: T.cost 0.083369, Train acc 0.962637, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 924: T.cost 0.083661, Train acc 0.962637, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 925: T.cost 0.083348, Train acc 0.962637, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 926: T.cost 0.083613, Train acc 0.962637, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 927: T.cost 0.083323, Train acc 0.962637, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 928: T.cost 0.083572, Train acc 0.962637, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 929: T.cost 0.083295, Train acc 0.962637, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 930: T.cost 0.083535, Train acc 0.962637, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 931: T.cost 0.083265, Train acc 0.962637, test acc 0.964912, took 0.00226 sec.\n",
      "Epoch 932: T.cost 0.083501, Train acc 0.962637, test acc 0.964912, took 0.00149 sec.\n",
      "Epoch 933: T.cost 0.083233, Train acc 0.962637, test acc 0.964912, took 0.00122 sec.\n",
      "Epoch 934: T.cost 0.083471, Train acc 0.962637, test acc 0.964912, took 0.00106 sec.\n",
      "Epoch 935: T.cost 0.083199, Train acc 0.962637, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 936: T.cost 0.083445, Train acc 0.962637, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 937: T.cost 0.083164, Train acc 0.962637, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 938: T.cost 0.083420, Train acc 0.962637, test acc 0.964912, took 0.00105 sec.\n",
      "New LR: 0.00311762709171\n",
      "Epoch 939: T.cost 0.083128, Train acc 0.962637, test acc 0.964912, took 0.00112 sec.\n",
      "Epoch 940: T.cost 0.083347, Train acc 0.962637, test acc 0.964912, took 0.00106 sec.\n",
      "Epoch 941: T.cost 0.082983, Train acc 0.962637, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 942: T.cost 0.083296, Train acc 0.962637, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 943: T.cost 0.082969, Train acc 0.962637, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 944: T.cost 0.083255, Train acc 0.962637, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 945: T.cost 0.082950, Train acc 0.962637, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 946: T.cost 0.083221, Train acc 0.962637, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 947: T.cost 0.082925, Train acc 0.962637, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 948: T.cost 0.083192, Train acc 0.962637, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 949: T.cost 0.082896, Train acc 0.962637, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 950: T.cost 0.083168, Train acc 0.962637, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 951: T.cost 0.082865, Train acc 0.962637, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 952: T.cost 0.083147, Train acc 0.962637, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 953: T.cost 0.082832, Train acc 0.962637, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 954: T.cost 0.083128, Train acc 0.962637, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 955: T.cost 0.082797, Train acc 0.962637, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 956: T.cost 0.083114, Train acc 0.962637, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 957: T.cost 0.082760, Train acc 0.962637, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 958: T.cost 0.083102, Train acc 0.962637, test acc 0.964912, took 0.00103 sec.\n",
      "New LR: 0.00308645072859\n",
      "Epoch 959: T.cost 0.082721, Train acc 0.962637, test acc 0.964912, took 0.00112 sec.\n",
      "Epoch 960: T.cost 0.083040, Train acc 0.962637, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 961: T.cost 0.082569, Train acc 0.962637, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 962: T.cost 0.083006, Train acc 0.962637, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 963: T.cost 0.082548, Train acc 0.962637, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 964: T.cost 0.082980, Train acc 0.962637, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 965: T.cost 0.082521, Train acc 0.962637, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 966: T.cost 0.082961, Train acc 0.962637, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 967: T.cost 0.082490, Train acc 0.962637, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 968: T.cost 0.082949, Train acc 0.962637, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 969: T.cost 0.082454, Train acc 0.962637, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 970: T.cost 0.082941, Train acc 0.962637, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 971: T.cost 0.082416, Train acc 0.962637, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 972: T.cost 0.082939, Train acc 0.962637, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 973: T.cost 0.082374, Train acc 0.962637, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 974: T.cost 0.082942, Train acc 0.962637, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 975: T.cost 0.082329, Train acc 0.962637, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 976: T.cost 0.082952, Train acc 0.962637, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 977: T.cost 0.082281, Train acc 0.962637, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 978: T.cost 0.082969, Train acc 0.962637, test acc 0.964912, took 0.00105 sec.\n",
      "New LR: 0.00305558623513\n",
      "Epoch 979: T.cost 0.082230, Train acc 0.962637, test acc 0.964912, took 0.00112 sec.\n",
      "Epoch 980: T.cost 0.082935, Train acc 0.962637, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 981: T.cost 0.082057, Train acc 0.962637, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 982: T.cost 0.082947, Train acc 0.962637, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 983: T.cost 0.082011, Train acc 0.962637, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 984: T.cost 0.082967, Train acc 0.962637, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 985: T.cost 0.081963, Train acc 0.962637, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 986: T.cost 0.082994, Train acc 0.962637, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 987: T.cost 0.081912, Train acc 0.962637, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 988: T.cost 0.083032, Train acc 0.962637, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 989: T.cost 0.081859, Train acc 0.962637, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 990: T.cost 0.083083, Train acc 0.962637, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 991: T.cost 0.081803, Train acc 0.962637, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 992: T.cost 0.083149, Train acc 0.962637, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 993: T.cost 0.081745, Train acc 0.962637, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 994: T.cost 0.083234, Train acc 0.962637, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 995: T.cost 0.081686, Train acc 0.962637, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 996: T.cost 0.083343, Train acc 0.962637, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 997: T.cost 0.081627, Train acc 0.964835, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 998: T.cost 0.083482, Train acc 0.962637, test acc 0.964912, took 0.00104 sec.\n",
      "New LR: 0.00302503038431\n",
      "Epoch 999: T.cost 0.081571, Train acc 0.964835, test acc 0.964912, took 0.00112 sec.\n",
      "Epoch 1000: T.cost 0.083577, Train acc 0.962637, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 1001: T.cost 0.081373, Train acc 0.964835, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1002: T.cost 0.083791, Train acc 0.960440, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1003: T.cost 0.081295, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1004: T.cost 0.084008, Train acc 0.962637, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1005: T.cost 0.081238, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1006: T.cost 0.084247, Train acc 0.962637, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1007: T.cost 0.081198, Train acc 0.969231, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1008: T.cost 0.084522, Train acc 0.962637, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1009: T.cost 0.081175, Train acc 0.969231, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1010: T.cost 0.084846, Train acc 0.960440, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1011: T.cost 0.081174, Train acc 0.969231, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1012: T.cost 0.085235, Train acc 0.962637, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1013: T.cost 0.081198, Train acc 0.969231, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1014: T.cost 0.085700, Train acc 0.960440, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1015: T.cost 0.081255, Train acc 0.969231, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 1016: T.cost 0.086256, Train acc 0.960440, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1017: T.cost 0.081351, Train acc 0.973626, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1018: T.cost 0.086917, Train acc 0.958242, test acc 0.964912, took 0.00104 sec.\n",
      "New LR: 0.00299478017958\n",
      "Epoch 1019: T.cost 0.081493, Train acc 0.971429, test acc 0.964912, took 0.00113 sec.\n",
      "Epoch 1020: T.cost 0.087532, Train acc 0.958242, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1021: T.cost 0.081437, Train acc 0.971429, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1022: T.cost 0.088371, Train acc 0.958242, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1023: T.cost 0.081431, Train acc 0.971429, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1024: T.cost 0.089053, Train acc 0.958242, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1025: T.cost 0.081431, Train acc 0.971429, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1026: T.cost 0.089594, Train acc 0.958242, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1027: T.cost 0.081411, Train acc 0.973626, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1028: T.cost 0.089997, Train acc 0.958242, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1029: T.cost 0.081356, Train acc 0.971429, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1030: T.cost 0.090251, Train acc 0.958242, test acc 0.964912, took 0.00106 sec.\n",
      "Epoch 1031: T.cost 0.081253, Train acc 0.971429, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1032: T.cost 0.090345, Train acc 0.958242, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1033: T.cost 0.081102, Train acc 0.971429, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1034: T.cost 0.090274, Train acc 0.958242, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1035: T.cost 0.080910, Train acc 0.969231, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1036: T.cost 0.090042, Train acc 0.958242, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1037: T.cost 0.080689, Train acc 0.971429, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1038: T.cost 0.089667, Train acc 0.958242, test acc 0.964912, took 0.00103 sec.\n",
      "New LR: 0.00296483239392\n",
      "Epoch 1039: T.cost 0.080459, Train acc 0.971429, test acc 0.964912, took 0.00112 sec.\n",
      "Epoch 1040: T.cost 0.088979, Train acc 0.958242, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 1041: T.cost 0.080013, Train acc 0.969231, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 1042: T.cost 0.088322, Train acc 0.960440, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1043: T.cost 0.079650, Train acc 0.969231, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1044: T.cost 0.087415, Train acc 0.962637, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1045: T.cost 0.079426, Train acc 0.969231, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1046: T.cost 0.086434, Train acc 0.962637, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1047: T.cost 0.079344, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1048: T.cost 0.085498, Train acc 0.964835, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1049: T.cost 0.079382, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1050: T.cost 0.084670, Train acc 0.962637, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1051: T.cost 0.079500, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1052: T.cost 0.083974, Train acc 0.964835, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1053: T.cost 0.079661, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1054: T.cost 0.083409, Train acc 0.962637, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1055: T.cost 0.079832, Train acc 0.964835, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1056: T.cost 0.082963, Train acc 0.962637, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1057: T.cost 0.079992, Train acc 0.964835, test acc 0.964912, took 0.00102 sec.\n",
      "Epoch 1058: T.cost 0.082615, Train acc 0.962637, test acc 0.964912, took 0.00104 sec.\n",
      "New LR: 0.00293518403079\n",
      "Epoch 1059: T.cost 0.080130, Train acc 0.964835, test acc 0.964912, took 0.00112 sec.\n",
      "Epoch 1060: T.cost 0.082279, Train acc 0.962637, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 1061: T.cost 0.080138, Train acc 0.962637, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1062: T.cost 0.082047, Train acc 0.962637, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1063: T.cost 0.080240, Train acc 0.962637, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1064: T.cost 0.081844, Train acc 0.962637, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1065: T.cost 0.080334, Train acc 0.962637, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1066: T.cost 0.081674, Train acc 0.962637, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1067: T.cost 0.080413, Train acc 0.962637, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1068: T.cost 0.081533, Train acc 0.962637, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1069: T.cost 0.080476, Train acc 0.962637, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1070: T.cost 0.081418, Train acc 0.962637, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1071: T.cost 0.080524, Train acc 0.962637, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 1072: T.cost 0.081324, Train acc 0.962637, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1073: T.cost 0.080557, Train acc 0.962637, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1074: T.cost 0.081248, Train acc 0.962637, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1075: T.cost 0.080579, Train acc 0.962637, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 1076: T.cost 0.081186, Train acc 0.962637, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1077: T.cost 0.080590, Train acc 0.962637, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1078: T.cost 0.081133, Train acc 0.962637, test acc 0.964912, took 0.00103 sec.\n",
      "New LR: 0.00290583209367\n",
      "Epoch 1079: T.cost 0.080593, Train acc 0.962637, test acc 0.964912, took 0.00112 sec.\n",
      "Epoch 1080: T.cost 0.081039, Train acc 0.962637, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1081: T.cost 0.080494, Train acc 0.962637, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1082: T.cost 0.080968, Train acc 0.962637, test acc 0.964912, took 0.00106 sec.\n",
      "Epoch 1083: T.cost 0.080515, Train acc 0.962637, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1084: T.cost 0.080907, Train acc 0.962637, test acc 0.964912, took 0.00109 sec.\n",
      "Epoch 1085: T.cost 0.080528, Train acc 0.962637, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1086: T.cost 0.080856, Train acc 0.962637, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1087: T.cost 0.080534, Train acc 0.962637, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1088: T.cost 0.080811, Train acc 0.962637, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1089: T.cost 0.080534, Train acc 0.962637, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1090: T.cost 0.080772, Train acc 0.962637, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1091: T.cost 0.080529, Train acc 0.962637, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1092: T.cost 0.080738, Train acc 0.962637, test acc 0.964912, took 0.00106 sec.\n",
      "Epoch 1093: T.cost 0.080521, Train acc 0.962637, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1094: T.cost 0.080707, Train acc 0.962637, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 1095: T.cost 0.080510, Train acc 0.962637, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1096: T.cost 0.080679, Train acc 0.962637, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1097: T.cost 0.080497, Train acc 0.962637, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1098: T.cost 0.080654, Train acc 0.962637, test acc 0.964912, took 0.00103 sec.\n",
      "New LR: 0.00287677381653\n",
      "Epoch 1099: T.cost 0.080482, Train acc 0.962637, test acc 0.964912, took 0.00111 sec.\n",
      "Epoch 1100: T.cost 0.080583, Train acc 0.962637, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 1101: T.cost 0.080371, Train acc 0.962637, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 1102: T.cost 0.080523, Train acc 0.962637, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1103: T.cost 0.080383, Train acc 0.962637, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1104: T.cost 0.080476, Train acc 0.962637, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1105: T.cost 0.080387, Train acc 0.962637, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1106: T.cost 0.080438, Train acc 0.962637, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1107: T.cost 0.080382, Train acc 0.962637, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1108: T.cost 0.080405, Train acc 0.962637, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1109: T.cost 0.080373, Train acc 0.962637, test acc 0.964912, took 0.00102 sec.\n",
      "Epoch 1110: T.cost 0.080377, Train acc 0.962637, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1111: T.cost 0.080361, Train acc 0.962637, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1112: T.cost 0.080352, Train acc 0.962637, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1113: T.cost 0.080346, Train acc 0.962637, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1114: T.cost 0.080328, Train acc 0.962637, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1115: T.cost 0.080330, Train acc 0.962637, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1116: T.cost 0.080306, Train acc 0.962637, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1117: T.cost 0.080312, Train acc 0.962637, test acc 0.964912, took 0.00249 sec.\n",
      "Epoch 1118: T.cost 0.080285, Train acc 0.962637, test acc 0.964912, took 0.0015 sec.\n",
      "New LR: 0.00284800597234\n",
      "Epoch 1119: T.cost 0.080294, Train acc 0.962637, test acc 0.964912, took 0.00128 sec.\n",
      "Epoch 1120: T.cost 0.080219, Train acc 0.962637, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 1121: T.cost 0.080182, Train acc 0.962637, test acc 0.964912, took 0.00107 sec.\n",
      "Epoch 1122: T.cost 0.080160, Train acc 0.962637, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1123: T.cost 0.080194, Train acc 0.962637, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1124: T.cost 0.080116, Train acc 0.962637, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1125: T.cost 0.080196, Train acc 0.962637, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 1126: T.cost 0.080081, Train acc 0.962637, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1127: T.cost 0.080189, Train acc 0.962637, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1128: T.cost 0.080052, Train acc 0.962637, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1129: T.cost 0.080177, Train acc 0.962637, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1130: T.cost 0.080027, Train acc 0.964835, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1131: T.cost 0.080162, Train acc 0.962637, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1132: T.cost 0.080004, Train acc 0.964835, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 1133: T.cost 0.080145, Train acc 0.964835, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1134: T.cost 0.079984, Train acc 0.964835, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1135: T.cost 0.080127, Train acc 0.964835, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1136: T.cost 0.079963, Train acc 0.964835, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1137: T.cost 0.080108, Train acc 0.964835, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 1138: T.cost 0.079944, Train acc 0.964835, test acc 0.964912, took 0.00103 sec.\n",
      "New LR: 0.00281952602556\n",
      "Epoch 1139: T.cost 0.080089, Train acc 0.964835, test acc 0.964912, took 0.00112 sec.\n",
      "Epoch 1140: T.cost 0.079880, Train acc 0.964835, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 1141: T.cost 0.079978, Train acc 0.964835, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1142: T.cost 0.079820, Train acc 0.964835, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1143: T.cost 0.079993, Train acc 0.964835, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1144: T.cost 0.079776, Train acc 0.964835, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 1145: T.cost 0.079994, Train acc 0.964835, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1146: T.cost 0.079742, Train acc 0.964835, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1147: T.cost 0.079986, Train acc 0.964835, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 1148: T.cost 0.079715, Train acc 0.964835, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1149: T.cost 0.079973, Train acc 0.964835, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1150: T.cost 0.079692, Train acc 0.964835, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1151: T.cost 0.079957, Train acc 0.964835, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 1152: T.cost 0.079671, Train acc 0.964835, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1153: T.cost 0.079939, Train acc 0.964835, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1154: T.cost 0.079651, Train acc 0.964835, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1155: T.cost 0.079921, Train acc 0.964835, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1156: T.cost 0.079631, Train acc 0.964835, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1157: T.cost 0.079903, Train acc 0.964835, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1158: T.cost 0.079611, Train acc 0.964835, test acc 0.964912, took 0.00104 sec.\n",
      "New LR: 0.00279133074917\n",
      "Epoch 1159: T.cost 0.079884, Train acc 0.964835, test acc 0.964912, took 0.00112 sec.\n",
      "Epoch 1160: T.cost 0.079549, Train acc 0.964835, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 1161: T.cost 0.079776, Train acc 0.964835, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1162: T.cost 0.079487, Train acc 0.964835, test acc 0.964912, took 0.00106 sec.\n",
      "Epoch 1163: T.cost 0.079792, Train acc 0.964835, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1164: T.cost 0.079443, Train acc 0.964835, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1165: T.cost 0.079794, Train acc 0.964835, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1166: T.cost 0.079409, Train acc 0.964835, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1167: T.cost 0.079786, Train acc 0.964835, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1168: T.cost 0.079383, Train acc 0.964835, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1169: T.cost 0.079773, Train acc 0.964835, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1170: T.cost 0.079361, Train acc 0.964835, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1171: T.cost 0.079756, Train acc 0.964835, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1172: T.cost 0.079341, Train acc 0.964835, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1173: T.cost 0.079738, Train acc 0.964835, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1174: T.cost 0.079322, Train acc 0.964835, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1175: T.cost 0.079719, Train acc 0.964835, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 1176: T.cost 0.079303, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1177: T.cost 0.079701, Train acc 0.964835, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1178: T.cost 0.079283, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "New LR: 0.00276341737714\n",
      "Epoch 1179: T.cost 0.079683, Train acc 0.964835, test acc 0.964912, took 0.00112 sec.\n",
      "Epoch 1180: T.cost 0.079223, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1181: T.cost 0.079577, Train acc 0.964835, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 1182: T.cost 0.079158, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1183: T.cost 0.079596, Train acc 0.964835, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1184: T.cost 0.079113, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1185: T.cost 0.079599, Train acc 0.964835, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1186: T.cost 0.079081, Train acc 0.967033, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 1187: T.cost 0.079590, Train acc 0.964835, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1188: T.cost 0.079055, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1189: T.cost 0.079576, Train acc 0.964835, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1190: T.cost 0.079034, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1191: T.cost 0.079558, Train acc 0.964835, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1192: T.cost 0.079015, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1193: T.cost 0.079539, Train acc 0.964835, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1194: T.cost 0.078997, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1195: T.cost 0.079521, Train acc 0.964835, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1196: T.cost 0.078978, Train acc 0.967033, test acc 0.964912, took 0.00102 sec.\n",
      "Epoch 1197: T.cost 0.079502, Train acc 0.964835, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1198: T.cost 0.078959, Train acc 0.967033, test acc 0.964912, took 0.00102 sec.\n",
      "New LR: 0.00273578314343\n",
      "Epoch 1199: T.cost 0.079486, Train acc 0.964835, test acc 0.964912, took 0.00111 sec.\n",
      "Epoch 1200: T.cost 0.078900, Train acc 0.967033, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 1201: T.cost 0.079382, Train acc 0.964835, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1202: T.cost 0.078834, Train acc 0.967033, test acc 0.964912, took 0.00102 sec.\n",
      "Epoch 1203: T.cost 0.079403, Train acc 0.964835, test acc 0.964912, took 0.00106 sec.\n",
      "Epoch 1204: T.cost 0.078789, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1205: T.cost 0.079406, Train acc 0.964835, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1206: T.cost 0.078757, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1207: T.cost 0.079397, Train acc 0.964835, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1208: T.cost 0.078733, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1209: T.cost 0.079381, Train acc 0.964835, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1210: T.cost 0.078713, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1211: T.cost 0.079362, Train acc 0.964835, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1212: T.cost 0.078695, Train acc 0.967033, test acc 0.964912, took 0.00102 sec.\n",
      "Epoch 1213: T.cost 0.079342, Train acc 0.964835, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1214: T.cost 0.078678, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1215: T.cost 0.079322, Train acc 0.964835, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1216: T.cost 0.078661, Train acc 0.967033, test acc 0.964912, took 0.00102 sec.\n",
      "Epoch 1217: T.cost 0.079303, Train acc 0.964835, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1218: T.cost 0.078642, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "New LR: 0.00270842528204\n",
      "Epoch 1219: T.cost 0.079286, Train acc 0.964835, test acc 0.964912, took 0.00111 sec.\n",
      "Epoch 1220: T.cost 0.078586, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1221: T.cost 0.079184, Train acc 0.964835, test acc 0.964912, took 0.00102 sec.\n",
      "Epoch 1222: T.cost 0.078519, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1223: T.cost 0.079206, Train acc 0.964835, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1224: T.cost 0.078475, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1225: T.cost 0.079208, Train acc 0.964835, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1226: T.cost 0.078444, Train acc 0.967033, test acc 0.964912, took 0.00102 sec.\n",
      "Epoch 1227: T.cost 0.079197, Train acc 0.964835, test acc 0.964912, took 0.00102 sec.\n",
      "Epoch 1228: T.cost 0.078421, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1229: T.cost 0.079179, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1230: T.cost 0.078404, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1231: T.cost 0.079158, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1232: T.cost 0.078388, Train acc 0.967033, test acc 0.964912, took 0.00102 sec.\n",
      "Epoch 1233: T.cost 0.079136, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1234: T.cost 0.078373, Train acc 0.967033, test acc 0.964912, took 0.00102 sec.\n",
      "Epoch 1235: T.cost 0.079114, Train acc 0.967033, test acc 0.964912, took 0.00102 sec.\n",
      "Epoch 1236: T.cost 0.078357, Train acc 0.967033, test acc 0.964912, took 0.00102 sec.\n",
      "Epoch 1237: T.cost 0.079094, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1238: T.cost 0.078340, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "New LR: 0.00268134102691\n",
      "Epoch 1239: T.cost 0.079076, Train acc 0.967033, test acc 0.964912, took 0.00111 sec.\n",
      "Epoch 1240: T.cost 0.078286, Train acc 0.967033, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 1241: T.cost 0.078974, Train acc 0.964835, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1242: T.cost 0.078220, Train acc 0.967033, test acc 0.964912, took 0.00102 sec.\n",
      "Epoch 1243: T.cost 0.078995, Train acc 0.967033, test acc 0.964912, took 0.00102 sec.\n",
      "Epoch 1244: T.cost 0.078176, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1245: T.cost 0.078995, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1246: T.cost 0.078147, Train acc 0.967033, test acc 0.964912, took 0.00102 sec.\n",
      "Epoch 1247: T.cost 0.078981, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1248: T.cost 0.078127, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1249: T.cost 0.078960, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1250: T.cost 0.078112, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1251: T.cost 0.078936, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1252: T.cost 0.078099, Train acc 0.967033, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 1253: T.cost 0.078911, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1254: T.cost 0.078086, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1255: T.cost 0.078886, Train acc 0.967033, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 1256: T.cost 0.078073, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1257: T.cost 0.078863, Train acc 0.967033, test acc 0.964912, took 0.00102 sec.\n",
      "Epoch 1258: T.cost 0.078059, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "New LR: 0.00265452761203\n",
      "Epoch 1259: T.cost 0.078843, Train acc 0.967033, test acc 0.964912, took 0.00113 sec.\n",
      "Epoch 1260: T.cost 0.078007, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1261: T.cost 0.078739, Train acc 0.967033, test acc 0.964912, took 0.00102 sec.\n",
      "Epoch 1262: T.cost 0.077943, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1263: T.cost 0.078758, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1264: T.cost 0.077901, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1265: T.cost 0.078756, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1266: T.cost 0.077875, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1267: T.cost 0.078739, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1268: T.cost 0.077857, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1269: T.cost 0.078715, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1270: T.cost 0.077845, Train acc 0.967033, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 1271: T.cost 0.078687, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1272: T.cost 0.077834, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1273: T.cost 0.078659, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1274: T.cost 0.077824, Train acc 0.967033, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 1275: T.cost 0.078631, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1276: T.cost 0.077813, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1277: T.cost 0.078605, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1278: T.cost 0.077801, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "New LR: 0.00262798227137\n",
      "Epoch 1279: T.cost 0.078582, Train acc 0.967033, test acc 0.964912, took 0.00112 sec.\n",
      "Epoch 1280: T.cost 0.077752, Train acc 0.967033, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 1281: T.cost 0.078477, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1282: T.cost 0.077690, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1283: T.cost 0.078493, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1284: T.cost 0.077651, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1285: T.cost 0.078486, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1286: T.cost 0.077627, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1287: T.cost 0.078467, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1288: T.cost 0.077612, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1289: T.cost 0.078440, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1290: T.cost 0.077602, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1291: T.cost 0.078410, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1292: T.cost 0.077594, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1293: T.cost 0.078378, Train acc 0.967033, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 1294: T.cost 0.077586, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1295: T.cost 0.078348, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1296: T.cost 0.077578, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1297: T.cost 0.078320, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1298: T.cost 0.077568, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "New LR: 0.0026017024694\n",
      "Epoch 1299: T.cost 0.078294, Train acc 0.967033, test acc 0.964912, took 0.00112 sec.\n",
      "Epoch 1300: T.cost 0.077522, Train acc 0.967033, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 1301: T.cost 0.078186, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1302: T.cost 0.077463, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1303: T.cost 0.078198, Train acc 0.967033, test acc 0.964912, took 0.00135 sec.\n",
      "Epoch 1304: T.cost 0.077428, Train acc 0.967033, test acc 0.964912, took 0.00136 sec.\n",
      "Epoch 1305: T.cost 0.078189, Train acc 0.967033, test acc 0.964912, took 0.00151 sec.\n",
      "Epoch 1306: T.cost 0.077406, Train acc 0.967033, test acc 0.964912, took 0.00106 sec.\n",
      "Epoch 1307: T.cost 0.078168, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1308: T.cost 0.077393, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1309: T.cost 0.078139, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1310: T.cost 0.077384, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1311: T.cost 0.078107, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1312: T.cost 0.077378, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1313: T.cost 0.078075, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1314: T.cost 0.077372, Train acc 0.967033, test acc 0.964912, took 0.00102 sec.\n",
      "Epoch 1315: T.cost 0.078043, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1316: T.cost 0.077366, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1317: T.cost 0.078013, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1318: T.cost 0.077358, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "New LR: 0.0025756854401\n",
      "Epoch 1319: T.cost 0.077985, Train acc 0.967033, test acc 0.964912, took 0.00112 sec.\n",
      "Epoch 1320: T.cost 0.077312, Train acc 0.967033, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 1321: T.cost 0.077875, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1322: T.cost 0.077258, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1323: T.cost 0.077884, Train acc 0.967033, test acc 0.964912, took 0.00102 sec.\n",
      "Epoch 1324: T.cost 0.077224, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1325: T.cost 0.077874, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1326: T.cost 0.077204, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1327: T.cost 0.077852, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1328: T.cost 0.077192, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1329: T.cost 0.077824, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1330: T.cost 0.077184, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1331: T.cost 0.077793, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1332: T.cost 0.077178, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1333: T.cost 0.077760, Train acc 0.967033, test acc 0.964912, took 0.00102 sec.\n",
      "Epoch 1334: T.cost 0.077173, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1335: T.cost 0.077729, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1336: T.cost 0.077167, Train acc 0.967033, test acc 0.964912, took 0.00102 sec.\n",
      "Epoch 1337: T.cost 0.077699, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1338: T.cost 0.077159, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "New LR: 0.00254992864793\n",
      "Epoch 1339: T.cost 0.077670, Train acc 0.967033, test acc 0.964912, took 0.00112 sec.\n",
      "Epoch 1340: T.cost 0.077114, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1341: T.cost 0.077560, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1342: T.cost 0.077063, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1343: T.cost 0.077567, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1344: T.cost 0.077030, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1345: T.cost 0.077557, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1346: T.cost 0.077011, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1347: T.cost 0.077537, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1348: T.cost 0.076998, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1349: T.cost 0.077510, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1350: T.cost 0.076990, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1351: T.cost 0.077480, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1352: T.cost 0.076984, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1353: T.cost 0.077449, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1354: T.cost 0.076979, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1355: T.cost 0.077419, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1356: T.cost 0.076973, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1357: T.cost 0.077389, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1358: T.cost 0.076966, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "New LR: 0.00252442932688\n",
      "Epoch 1359: T.cost 0.077361, Train acc 0.967033, test acc 0.964912, took 0.00113 sec.\n",
      "Epoch 1360: T.cost 0.076920, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1361: T.cost 0.077251, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1362: T.cost 0.076870, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1363: T.cost 0.077258, Train acc 0.967033, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 1364: T.cost 0.076839, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1365: T.cost 0.077249, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1366: T.cost 0.076818, Train acc 0.967033, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 1367: T.cost 0.077230, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1368: T.cost 0.076805, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1369: T.cost 0.077205, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1370: T.cost 0.076796, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1371: T.cost 0.077177, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1372: T.cost 0.076789, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1373: T.cost 0.077149, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1374: T.cost 0.076783, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1375: T.cost 0.077120, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1376: T.cost 0.076776, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1377: T.cost 0.077092, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1378: T.cost 0.076768, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "New LR: 0.00249918494141\n",
      "Epoch 1379: T.cost 0.077065, Train acc 0.967033, test acc 0.964912, took 0.00112 sec.\n",
      "Epoch 1380: T.cost 0.076721, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1381: T.cost 0.076957, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1382: T.cost 0.076673, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1383: T.cost 0.076963, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1384: T.cost 0.076641, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1385: T.cost 0.076956, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1386: T.cost 0.076620, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1387: T.cost 0.076939, Train acc 0.967033, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 1388: T.cost 0.076606, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1389: T.cost 0.076916, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1390: T.cost 0.076595, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1391: T.cost 0.076891, Train acc 0.967033, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 1392: T.cost 0.076587, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1393: T.cost 0.076864, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1394: T.cost 0.076579, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1395: T.cost 0.076837, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1396: T.cost 0.076572, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1397: T.cost 0.076811, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1398: T.cost 0.076563, Train acc 0.967033, test acc 0.964912, took 0.00105 sec.\n",
      "New LR: 0.0024741931865\n",
      "Epoch 1399: T.cost 0.076786, Train acc 0.967033, test acc 0.964912, took 0.00111 sec.\n",
      "Epoch 1400: T.cost 0.076515, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1401: T.cost 0.076679, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1402: T.cost 0.076468, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1403: T.cost 0.076686, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1404: T.cost 0.076435, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1405: T.cost 0.076679, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1406: T.cost 0.076413, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1407: T.cost 0.076664, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1408: T.cost 0.076398, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1409: T.cost 0.076644, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1410: T.cost 0.076386, Train acc 0.967033, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 1411: T.cost 0.076620, Train acc 0.967033, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 1412: T.cost 0.076377, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1413: T.cost 0.076595, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1414: T.cost 0.076368, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1415: T.cost 0.076570, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1416: T.cost 0.076359, Train acc 0.967033, test acc 0.964912, took 0.00102 sec.\n",
      "Epoch 1417: T.cost 0.076545, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1418: T.cost 0.076350, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "New LR: 0.00244945129612\n",
      "Epoch 1419: T.cost 0.076521, Train acc 0.967033, test acc 0.964912, took 0.00112 sec.\n",
      "Epoch 1420: T.cost 0.076302, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1421: T.cost 0.076415, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1422: T.cost 0.076255, Train acc 0.967033, test acc 0.964912, took 0.00102 sec.\n",
      "Epoch 1423: T.cost 0.076423, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1424: T.cost 0.076222, Train acc 0.967033, test acc 0.964912, took 0.00106 sec.\n",
      "Epoch 1425: T.cost 0.076417, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1426: T.cost 0.076199, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1427: T.cost 0.076404, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1428: T.cost 0.076183, Train acc 0.967033, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 1429: T.cost 0.076385, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1430: T.cost 0.076170, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1431: T.cost 0.076362, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1432: T.cost 0.076160, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1433: T.cost 0.076339, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1434: T.cost 0.076151, Train acc 0.967033, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 1435: T.cost 0.076315, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1436: T.cost 0.076142, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1437: T.cost 0.076291, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1438: T.cost 0.076132, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "New LR: 0.00242495673476\n",
      "Epoch 1439: T.cost 0.076267, Train acc 0.967033, test acc 0.964912, took 0.00114 sec.\n",
      "Epoch 1440: T.cost 0.076083, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1441: T.cost 0.076163, Train acc 0.967033, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 1442: T.cost 0.076037, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1443: T.cost 0.076171, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1444: T.cost 0.076004, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1445: T.cost 0.076166, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1446: T.cost 0.075981, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1447: T.cost 0.076153, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1448: T.cost 0.075964, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1449: T.cost 0.076135, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1450: T.cost 0.075951, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1451: T.cost 0.076114, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1452: T.cost 0.075940, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1453: T.cost 0.076091, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1454: T.cost 0.075931, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1455: T.cost 0.076067, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1456: T.cost 0.075922, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1457: T.cost 0.076044, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1458: T.cost 0.075912, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "New LR: 0.00240070719738\n",
      "Epoch 1459: T.cost 0.076021, Train acc 0.967033, test acc 0.964912, took 0.00112 sec.\n",
      "Epoch 1460: T.cost 0.075863, Train acc 0.967033, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 1461: T.cost 0.075918, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1462: T.cost 0.075817, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1463: T.cost 0.075926, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1464: T.cost 0.075784, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1465: T.cost 0.075922, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1466: T.cost 0.075761, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1467: T.cost 0.075910, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1468: T.cost 0.075744, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1469: T.cost 0.075892, Train acc 0.967033, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 1470: T.cost 0.075731, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1471: T.cost 0.075872, Train acc 0.967033, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 1472: T.cost 0.075720, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1473: T.cost 0.075849, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1474: T.cost 0.075710, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1475: T.cost 0.075826, Train acc 0.967033, test acc 0.964912, took 0.00102 sec.\n",
      "Epoch 1476: T.cost 0.075701, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1477: T.cost 0.075803, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1478: T.cost 0.075691, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "New LR: 0.00237670014845\n",
      "Epoch 1479: T.cost 0.075781, Train acc 0.967033, test acc 0.964912, took 0.00112 sec.\n",
      "Epoch 1480: T.cost 0.075643, Train acc 0.967033, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 1481: T.cost 0.075680, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1482: T.cost 0.075597, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1483: T.cost 0.075688, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1484: T.cost 0.075564, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1485: T.cost 0.075684, Train acc 0.967033, test acc 0.964912, took 0.00102 sec.\n",
      "Epoch 1486: T.cost 0.075540, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1487: T.cost 0.075672, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1488: T.cost 0.075523, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1489: T.cost 0.075655, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1490: T.cost 0.075510, Train acc 0.967033, test acc 0.964912, took 0.00269 sec.\n",
      "Epoch 1491: T.cost 0.075635, Train acc 0.967033, test acc 0.964912, took 0.0016 sec.\n",
      "Epoch 1492: T.cost 0.075499, Train acc 0.967033, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 1493: T.cost 0.075613, Train acc 0.967033, test acc 0.964912, took 0.00105 sec.\n",
      "Epoch 1494: T.cost 0.075489, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1495: T.cost 0.075591, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1496: T.cost 0.075480, Train acc 0.967033, test acc 0.964912, took 0.00103 sec.\n",
      "Epoch 1497: T.cost 0.075568, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "Epoch 1498: T.cost 0.075471, Train acc 0.967033, test acc 0.964912, took 0.00104 sec.\n",
      "New LR: 0.00235293305246\n",
      "Epoch 1499: T.cost 0.075546, Train acc 0.967033, test acc 0.964912, took 0.00112 sec.\n",
      "\n",
      "Total time spent: 1.6092 seconds\n",
      "Traing Acc: 0.967032967033\n",
      "Test Acc: 0.964912280702\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(formatter={'float': '{: 0.4f}'.format}, suppress=True)\n",
    "train_accs, test_accs = [], []\n",
    "total_time = 0\n",
    "param_outputs, disc_outputs = [], []\n",
    "disc_dist_t_1 = None\n",
    "quantized_bins = []\n",
    "try:\n",
    "    for n in range(NUM_EPOCHS):\n",
    "        start_time = time.time()\n",
    "        train_cost, train_acc, param_output, disc_output = train_epoch(data['X_train'], data['y_train'])\n",
    "        test_acc = eval_epoch(data['X_test'], data['y_test'])\n",
    "        test_accs += [test_acc]\n",
    "        train_accs += [train_acc]\n",
    "\n",
    "        if DISC:\n",
    "            param_outputs = np.append(param_outputs, param_output)\n",
    "            disc_outputs = np.append(disc_outputs, disc_output)\n",
    "\n",
    "        if (n+1) % 20 == 0:\n",
    "            new_lr = sh_lr.get_value() * 0.99\n",
    "            print \"New LR:\", new_lr\n",
    "            sh_lr.set_value(lasagne.utils.floatX(new_lr))\n",
    "        \n",
    "        # Non-uniform Quantization\n",
    "        if DISC:\n",
    "            if n>0 and np.mod(n, 10) == 0:\n",
    "                dist = disc_output.reshape((-1, QUANT_UNIT))\n",
    "                q_bins = find_quantization_bins(dist, sharedBins=sharedBins)\n",
    "                quantized_bins.append(q_bins)\n",
    "\n",
    "        time_spent = time.time() - start_time\n",
    "        total_time += time_spent\n",
    "        print \"Epoch {0}: T.cost {1:0.6f}, Train acc {2:0.6f}, test acc {3:0.6f}, took {4:.3} sec.\".format(\n",
    "                n, train_cost, train_acc, test_acc, time_spent)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "print \"\\nTotal time spent: {0:.5} seconds\\nTraing Acc: {1}\\nTest Acc: {2}\\n\".format(total_time, train_acc, test_acc) \n",
    "\n",
    "if DISC:\n",
    "    story = {'train_accs': train_accs,\n",
    "             'test_accs': test_accs,\n",
    "             'epoch_reached': n, \n",
    "             'total_time': total_time,\n",
    "             'disc_enabled': DISC,\n",
    "             'learning_rate': LEARNING_RATE,\n",
    "             'batch_size': BATCH_SIZE,\n",
    "             'dense_params': param_output,\n",
    "             'disc_params': disc_output,\n",
    "             'quantized_bins': quantized_bins}\n",
    "else:\n",
    "    story = {'train_accs': train_accs,\n",
    "             'test_accs': test_accs,\n",
    "             'epoch_reached': n, \n",
    "             'total_time': total_time,\n",
    "             'disc_enabled': DISC,\n",
    "             'learning_rate': LEARNING_RATE,\n",
    "             'batch_size': BATCH_SIZE,\n",
    "             'disc_params': disc_output}   \n",
    "\n",
    "with open(TEST_NAME + '.model', 'wb') as fp:\n",
    "  pickle.dump(story, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HISTOGRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdUAAAHDCAYAAAByNoBxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f80143c9a50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(1-np.array(train_accs), label='Training Error')\n",
    "plt.plot(1-np.array(test_accs), label='Test Error')\n",
    "plt.legend(fontsize=20)\n",
    "plt.xlabel('Epoch', fontsize=8)\n",
    "plt.ylabel('Error', fontsize=8)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:theano]",
   "language": "python",
   "name": "conda-env-theano-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
